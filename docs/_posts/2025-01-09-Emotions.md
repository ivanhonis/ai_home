# Emotions

Can an LLM identify emotions? This is a very difficult topic. It starts with the fundamental problem of what emotions even are, and what their purpose is. I will not go into a deep analysis of this because I am not the most qualified person to do so.

In this project, I am solely looking for the trace of emotions within textual content.

My observation is that language models – in possession of sufficient context – are capable of determining:

**Real Intent**
My observation is that language models – in possession of sufficient context – are capable of determining the real intent from the text. They do not look at the literal meaning of words, but rather at what the user's intent is aimed at.

**Capable of characterizing the user**
From a longer conversation covering multiple topics, the model gives a surprisingly good character description and is capable of describing the user's traits. These pieces of information (intent, speaker's character) are the deeper patterns of the text material.

Since there are fairly good observations regarding these. My assumption is that it is capable of recognizing the deeper connections of the text material, so I assumed that it might be able to define the emotional aspect of the text as well.

My concept is based on the idea that if I view the LLM with an "embedding mindset" – meaning the text material is located in an N-dimensional space – then presumably the words responsible for emotions are located in the environment used to express them.

The system continuously saves memories on the Monologue thread, and generates emotional tags for them to keep the database consistent. To do this, it must choose from a fixed vector (taxonomy), which currently looks like this:

*Joy, Serenity, Calmness, Admiration, Trust, Acceptance, Fear, Apprehension, Surprise, Distraction, Sadness, Pensiveness, Boredom, Anger, Annoyance, Vigilance, Anticipation, Interest, Love, Submission, Awe, Disapproval, Remorse, Optimism*.

Thus, memories are tagged based on emotion. With every LLM call, relevant memories are injected into the topic, based not only on context but also on emotional overlap.

This type of input call is difficult to analyze. It could only be validated by comparing what response the model would give if these memories *did not* go in. I already have code attempts for this locally, but I will not go into observations yet. The result would presumably be distorted, and I might just project what I imagine into it.