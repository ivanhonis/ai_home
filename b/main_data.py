import json
from pathlib import Path
from typing import Dict, Any, List, Tuple

# --------------------------------------------------------------------
# MODULE IMPORTS
# --------------------------------------------------------------------
# UPDATED: rooms -> modes
from engine import (
    identity as ident_lib,
    context as ctx_lib,
    modes,
    files,
)

# --------------------------------------------------------------------
# GLOBAL VARIABLES AND CONFIGURATION
# --------------------------------------------------------------------

BASE_DIR = Path(__file__).resolve().parent  # b/
ROOT_DIR = BASE_DIR.parent  # Project root
SLOT_CONFIG_PATH = ROOT_DIR / "slot.json"

# Role Name Mapping
ROLE_NAME_MAP = {
    1: "FIRST SELF",
    2: "SECOND SELF",
    3: "THIRD SELF"
}

# Inactivity time to trigger proactive thinking
PROACTIVE_INTERVAL_SECONDS = 60.0 * 15  # 15 minutes

# --- SUBCONSCIOUS / INTERNAL MONOLOGUE CONFIGURATION ---
INTERNAL_LOG_FILE = "log_for_internal.json"
INTERNAL_MEMOS_FILE = "internal_memos.json"
INTERNAL_MONOLOGUE_OUTPUT_FILE = "internal_monologue.json"

# Frequency of the 3rd thread (in seconds)
MONOLOGUE_INTERVAL_SECONDS = 25.0
MONOLOGUE_KEEP_COUNT = 1

# --- MEMORY CONFIGURATION ---
RELEVANT_MEMORY_FILE = "relevant_memory.json"


def load_slot_meta() -> Tuple[int, str]:
    """
    Loads the Role ID and Version (Generation) from slot.json.
    """
    config = files.load_json(SLOT_CONFIG_PATH, default={})
    slots = config.get("slots", {})
    slot_cfg = slots.get(BASE_DIR.name, {})
    role_id = slot_cfg.get("role", 2)
    version = slot_cfg.get("version", "E?")
    return role_id, version


# Load variables at the start of execution
ROLE_ID, GENERATION = load_slot_meta()


# --------------------------------------------------------------------
# DATA LOADING HELPERS
# --------------------------------------------------------------------

def load_relevant_memories(mode_id: str) -> List[Dict[str, Any]]:
    """
    Loads the vector-relevant memories generated by the Memory Thread.
    The file is located in the mode's directory.
    UPDATED: room logic -> mode logic
    """
    mode_path = modes.get_mode_path(mode_id)
    mem_path = mode_path / RELEVANT_MEMORY_FILE

    if mem_path.exists():
        try:
            with mem_path.open("r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return []
    return []


def load_all_context_data(mode_id: str) -> Dict[str, Any]:
    """
    Gathers all data required for prompt generation:
    - Identity
    - Hybrid Memory (Relevant memories - RAG)
    - Tool usage data (use.json)
    - Context (history)
    UPDATED: room_id -> mode_id
    """
    # 1. Global Data
    ident_data = ident_lib.load_identity()
    use_data = files.load_json("use.json", [])

    # 2. Global Context Tail (Continuity)
    # UPDATED: 'nappali' -> 'general'
    global_ctx_tail = []
    if mode_id != "general":
        full_global_ctx = ctx_lib.load_context("general")
        if full_global_ctx:
            global_ctx_tail = full_global_ctx[-5:]  # Last 5 messages

    # 3. Local Data (Mode)
    local_ctx = ctx_lib.load_context(mode_id)

    # --- Relevant Memories (Result of Memory Loop - DB based) ---
    relevant_memories = load_relevant_memories(mode_id)

    # --- Load Internal Monologue (so the Worker can see it) ---
    monologue_data = files.load_json(INTERNAL_MONOLOGUE_OUTPUT_FILE, {})

    return {
        "identity": ident_data,
        "relevant_memories": relevant_memories,
        "use_data": use_data,
        "global_context_tail": global_ctx_tail,
        "local_context": local_ctx,
        "monologue_data": monologue_data
    }