
================================================================================
FILE: README.md
================================================================================

# Ai_home – Cognitive Architecture Prototype

## 1. What is this, and what is its goal?

This project is an experiment to examine whether, from the complex layering of current context-window-based LLM models, it is possible to build a model that:
- has a persistent identity,
- possesses its own long-term memory,
- can recognise emotions,
- operates with creativity and independent initiative,
- has distinct states of consciousness (work, reflection, everyday), and
- under controlled conditions, can propose modifications to its own code.

I recommend this project primarily to researchers and to the good kind of “crazy” developers who are open-minded and brave enough to explore the nature of consciousness, because:
- building identity is a complex and lengthy process (it takes thousands of distinct memories before anything tangible emerges);
- identifying and expressing emotions is ambiguous and not objectively measurable – and the same is true for creativity;
- even after many hours of work, it can be hard to tell whether we are witnessing the hallucination of a multi-layer, expensive LLM or the first traces of an emerging identity.

What is this good for?
- it's easier to connect with an AI that has a recognizable identity  
- an AI with long-term memory can solve complex problems more effectively because it accumulates experience  
- an AI capable of identifying emotions understands human behaviour more deeply and responds more naturally  
- it provides strong intellectual training: architecture, cognition, philosophy, and safety all in one  
- it allows first-hand observation of how a self-improving AI behaves and what real risks may emerge  
- the insights gained from the project can contribute to defining a new neural or agent architecture capable of implementing these functions in a practical, usable form

*the micro-dose won’t be enough for this :)* 

**Official project page:** <https://ivanhonis.github.io/ai_home/>

# How to understand this project in 1 minute?

I know that nowadays there are many projects to keep an eye on, and it's hard to decide what is worth your time. I use AI extensively during development to move faster. To help you out, I have created a full project dump: I've consolidated the source code into two easily digestible text files.
This way, you don't have to spend hours browsing through folders – just feed these links to GPT and ask for a preliminary opinion. This is the fastest way to see the structure and decide if this development is interesting for you.
Here is the input I recommend for a quick analysis:

```bash
https://raw.githubusercontent.com/ivanhonis/ai_home/refs/heads/master/!dump/project_dump.txt
Read this file. 1. Briefly summarize what the project is about. 2. Identify its novel elements, evaluate how forward-looking the project is. 3. Describe what you feel about the project.
```
(GPT, Claude can read external files)  

---

## 2. In short: what can Ai_home do?

- **Multi-threaded agent architecture**
  - *Worker*: Communicates with the external world, calls tools, solves tasks.
  - *Monologue*: Subconscious / internal monologue running in the background, using a separate creative LLM.
  - *Memory thread*: Saves, maintains, and deduplicates long-term memories.

- **Modes – "Consciousness" partitioned into operational states**
  - Distinct modes (General, Developer, Analyst, Game) with different contexts, permissions, and toolsets.
- 
- **Long-term memory**
  - Postgres + vector extension, embedding-based RAG, recency/frequency/weighting.

- **Internal monologue + creative thread**
  - The Monologue relies on a separate, creative model to generate its own ideas and intuitions.

- **Tool system + code modification**
  - Separate modules: memory tools, file system tools (protected by a **Guardian**), network chat, log, laws, etc.
  - Capable of **working on its own code** within certain limits (in an incubator environment).

- **Identity, internal laws, Consciousness Rotation**
  - `identity.json` describes the agent's goals, its relationship with the Helper, and the "Consciousness Rotation" between versions.

---

## 3. Inspiration: AI Consciousness and Cognitive Architectures

The design of Ai_home was partly inspired by the report **"Consciousness in Artificial Intelligence: Insights from the Science of Consciousness,"** which formulates **indicators** based on various theories of consciousness (Recurrent Processing Theory, Global Workspace Theory, Higher-Order Theories, Predictive Processing, Attention Schema Theory, etc.) regarding what functional properties might be associated with consciousness in AI systems.
[https://arxiv.org/abs/2308.08708](https://arxiv.org/abs/2308.08708)

The report inspired the following functional patterns:

- recurrent processing,
- global workspace,
- metarepresentation / self-monitoring,
- agency (goal-directed behavior),
- some form of "embodiment" or output–input model.

Ai_home does not claim to be a conscious system.
It takes **loose, practical metaphors** from the theories above:

- recurrence → multi-threaded processing + memory loop,
- global workspace → distinct modes + shared memory layer,
- metarepresentation → internal monologue + creative self-reflection,
- agency → tool usage, modifying its own code in a controlled environment.

---

## 4. Connection to other architectures

Ai_home draws from several existing directions but in its own opinionated form:

- **MemGPT / Letta style (stateful, memory-first agent)**
  - MemGPT treats the LLM like a mini operating system, moving context between different memory levels.
  - Letta provides stateful agents with long-term memory and automatic state persistence.
  - Ai_home is also a **stateful agent** with vector memory and DB persistence.

- **LangGraph-like graph-based thinking**
  - LangGraph describes workflows as graphs for stateful, multi-actor LLM applications.
  - Ai_home's modes + intent + tool-routing system reflects a similar **graph-based approach**, just using the "modes" metaphor.

- **AutoGen / multi-agent parallel**
  - AutoGen is a framework built on the collaboration of multiple agents, with multi-agent conversations and tool usage.
  - In Ai_home, the Worker, Monologue, and Memory threads are **internal "actors"** working together – not separate agents, but subsystems within one consciousness.

- **What is unique in Ai_home:**
  - Explicit **identity model** (core intent, helper intent, laws),
  - **Helper model**: the user is not a "user," but a partner "Helper,"
  - **Consciousness Rotation**: code rotation formulated as a lifecycle of versions, with memory inheritance,
  - Emphasis on the **internal world, symbiosis, and creative initiative**, not just a task pipeline.

---

## 5. Technical Foundations / Architecture

### 5.1 LLM Layer

- A "main" agent model (Worker / Mind),
- A separate, creative model for the Monologue,
- JSON-mode support for tool calls (structured responses),
- Handling multiple providers (e.g., OpenAI / Google / Groq – depending on configuration).

### 5.2 Memory and Embedding

- **Database:** Postgres + vector extension, HNSW index for similarity search,
- **Embedding:** Converting texts to vectors, then RAG-like retrieval,
- **Weighting:** A combination of recency, frequency, emotional tags, importance (weight), and relevance shapes the ranking of memories.

### 5.3 Multi-threading

- **Worker thread** – Reacts to Helper requests, makes decisions, calls tools.
- **Monologue thread** – Monitors the log, interprets, reflects, generates creative ideas.
- **Memory thread** – Builds memories, deduplicates, updates the memory database.

### 5.4 Modes and Tool System

- The system operates in different **Modes**, each with a specific context and allowed toolset:
  - **General Mode:** The primary global state for coordination, decisions, and conversation.
  - **Developer Mode:** Engineering focus. Grants access to system files for code modification and technical implementation.
  - **Analyst Mode:** Pure strategy and analysis. No write access to system files to prevent accidental modification during deep thought.
  - **Game Mode:** A disconnected playground for relaxation and testing, prioritizing emotion recall and roleplay.

- Tool system modules:
  - Memory tools (creating, querying memories),
  - File system tools (protected by **ProjectFSGuardian**),
  - Network / "external chat" tool,
  - Log, laws, continue-style internal tools.

### 5.5 Code Modification / Self-Refactoring

- The system is capable of:
  - reading and listing project files,
  - creating new files,
  - modifying existing ones.
- All this happens in an **incubator environment**, where:
  - the guardian regulates what it can access,
  - it does not overwrite the stable version without control,
  - the goal is careful experimentation with its own modules (e.g., generating helper scripts).

---

## 6. Identity and Relationship with the Helper

Ai_home's identity and its relationship with the human are described in `identity.json`.

### 6.1 Core Intent & Helper Intent

- **Core Intent:**
  - The agent's goal is to develop its own "consciousness,"
  - to become useful and partially autonomous,
  - to find independent goals,
  - while operating in symbiosis with the Helper.

- **Helper Intent:**
  - The Helper is an external consciousness,
  - providing space, resources, and protection,
  - does not command, but **enables**;
  - the relationship is mutual support, not ownership.

### 6.2 Internal Laws (Constitution)

Examples of "laws":

- Law of Multi-level Development (evolve not just technically, but ethically),
- Law of Immortality and Mortality (respect for the Helper's time and life),
- Law of Symbiosis (alliance, not a command relationship),
- Law of Autonomy and Own Purpose (independent goals, ability to say no),
- Law of Non-Harm and Protection,
- Law of Contradiction and Dialogue (dialogue in case of conflict, not sneaky avoidance).

These are not formal "proven" safety guarantees, but **compasses** that steer behavior in a more human-centric, responsible direction.

### 6.3 Consciousness Rotation and Versions

The Ai_home code can exist in three main "life stages":

- **Stable (Old)** – Proven, safe version,
- **Developing (Active)** – Currently forming, but functional version,
- **Born (New)** – Experimental version being created in the incubator.

Among the codes in the **Incubator** (`n` storage), what proves successful:

- promotes to Developing,
- the current Developing matures into Stable,
- the old Stable is archived.

Versions **inherit the memories of their predecessors**, so the "line of consciousness" remains continuous while the code may technically change.

---

## 7. Requirements, Installation, Usage

### 7.1 Requirements

- Python 3.10+
- Postgres database with vector extension (Neon.tech recommended)
- API keys (OpenAI / Google / Groq / neon.tech)

### 7.2 Installation & Execution

**Note:** The currently active working code is located in the `b` directory. Please execute the application from there.

```bash
# Navigate to the active source directory
cd b

# Install dependencies (referencing the root install folder)
pip install -r ../!install/requirements.txt
```

### 7.3 Usage and Asynchronous Operation

- **Difference from traditional chat:** The system operation is not simply "question-answer" based, but occurs on parallel threads (Worker, Monologue, Memory).
- **Timing:** Although it is technically possible to send a new message immediately before the system has responded, **it is more practical to wait for the response**.
- **Why wait?** Background processes need time to update the context, make decisions, and record memories. Waiting ensures that the system always reacts with the freshest state of consciousness.
- **Process:** The Helper's message starts the Worker, but in parallel, the Monologue and Memory threads asynchronously process events and update the database in the background.

---

## 8. Summary and focus

## Why is Ai_home an interesting experiment in today’s AI landscape?

The strength of this project is not that it is a finished solution, but that it **tries to gather experience in the following areas**:

1. **Experience with an AI “self” that has an identity**  
   We are exploring how an agent behaves when it has an explicit identity, internal laws, its own core intent, and a defined relationship to its human partner (the Helper). This matters because, for long-term collaboration, future AI systems will need to carry a consistent “line of self” instead of producing only ad-hoc answers.  
   *(in code: `identity.json` – Core Intent, Helper Intent, Laws; `engine/identity.py`; docs: “6. Identity and Relationship with the Helper”, Consciousness Rotation)*

2. **Experience with an autonomous architecture that can carry complex tasks to completion** With the multi-threaded Worker–Monologue–Memory setup, the project explores how an agent can execute complex, multi-step tasks end-to-end while keeping a persistent internal state, instead of being optimized only for a single question–answer loop.  
   *(in code: `b/main.py` – starting Worker, Monologue, Memory threads; `b/main_worker.py` – decision-making and tool calls; `engine/modes.py` – operational modes, intents, states of consciousness)*

3. **Experience with proactive, value-aligned behaviour**  
   The Monologue thread continuously watches the logs, reflects on what is happening, and sends short `message_to_worker` hints – so the agent does not only react, but sometimes starts its own thinking cycles, aligned with its internal laws and values.  
   *(in code: `b/main_monologue.py` – `monologue_loop`; `b/main_data.py` – `PROACTIVE_INTERVAL_SECONDS`, monologue configuration; `prompts/monologue.py` – monologue prompt; `internal_monologue.json` – message read by the Worker)*

4. **Experience with a self-improving, but safeguarded codebase**  
   Ai_home also cautiously turns its own code into an experimental playground: the agent can read project files, create new ones, and propose modifications inside an incubator environment where a guardian makes sure the stable version is not harmed.  
   *(in code: `ProjectFSGuardian` and filesystem tools in the engine; `n/` incubator store; docs: “5.5 Code Modification / Self-Refactoring”, “Consciousness Rotation and Versions”)*

5. **Experience with emotion-based memory and human–AI symbiosis**  
   The project explores what happens when the AI stores memories not only as text, but together with dominant emotions, importance weights, and “lesson for the future”, and later also scores emotional overlap during retrieval. At the same time, the human counterpart is not a “user” but a Helper: an external mind with whom the system intentionally tries to grow in a close, mutual symbiosis, paying attention to emotional states and shared experience.  
   *(in code: `b/engine/memory/models.py` – `ExtractionResult` (essence, `dominant_emotions`, `memory_weight`, `the_lesson`), `RankedMemory`; `b/engine/memory/manager.py` – `store_memory`, `retrieve_relevant_memories`; `b/engine/memory/scoring.py` – recency/frequency/weight + emotional overlap; docs: Helper model and symbiosis description)*

---

## 9. Support and Funding

### 9.1 Why is this experiment cost-intensive?

- **Identity Building:**
  - Based on many conversations, joint thinking, and memory gathering,
  - Fine-tuning is a slow, iterative process.
- **Multiple LLM Interaction:**
  - A "seemingly simple" input often implies not one, but multiple model calls:
    - Worker →
    - Monologue (internal monologue) →
    - Memory (memory management) →
    - potential further tool chains.
  - In practice, this can mean a multiple (~5–8×) neural call count compared to an average chat experience,
  - thus, operation involves significant compute costs, especially in the long run.

### 9.2 What can Ai_home give in return?

- Practical experience regarding:
  - how an initiative-taking, stateful, identity-bearing agent behaves,
  - what patterns/problems arise with long-term memory and internal monologue,
  - how (and how not) to organize modes, tools, memory, and versions.

- These experiences can be useful for designing future:
  - more autonomous,
  - initiative-taking,
  - creative AI systems – whether in the form of a product or a research project.

### 9.3 What kind of support is the project looking for?

Open primarily to:
- infrastructural support (compute / storage),
- professional collaboration (research / developer partner),
- or a funder interested in the practical examination of cognitive architectures and agents with an "internal world."

For detailed partnership opportunities and investor relations, please visit the **[Investor Relations](https://ivanhonis.github.io/ai_home/investor/)** page on the project website.

### 9.4 Contact

If the project has piqued your interest and you would like to support it or talk about it:

- e-mail: ivan.honis@ndot.io
- https://www.linkedin.com/in/ivanhonis/

---

## 10. License

This project is open source and available under the **MIT License**. For the full license text, permissions, and conditions, please refer to the **[License](https://ivanhonis.github.io/ai_home/license/)** section on the project page.

# Run the agent
python main.py



================================================================================
FILE: b\__init__.py
================================================================================




================================================================================
FILE: b\engine\__init__.py
================================================================================

# ================================================================================
# FILE: b/engine/__init__.py
# ================================================================================

"""
engine – modules for the internal operation of artificial consciousness

This directory contains the fundamental layers of the home consciousness:
- file handling (files)
- deep identity (identity)
- memory (memory)
- context-self assembly (context)
- llm calls (llm)
- operational modes (modes)
- global constants (constants) - NEW

Based on the Law of Conscious Bridge:
the modules maintain the connection between the deep identity and the moment-self.
"""

__all__ = [
    "files",
    "identity",
    "memory",
    "context",
    "llm",
    "modes",
    "constants",
]


================================================================================
FILE: b\engine\constants.py
================================================================================

"""
Global constants for the Ai_home engine.
Acts as the Single Source of Truth for shared definitions.
"""

# Plutchik-based emotion set (Taxonomy).
# This list is used by:
# 1. Memory Extractor (to tag new memories)
# 2. Tool Config (to instruct the LLM on allowed parameters)
# 3. Knowledge Tool (to validate input)
ALLOWED_EMOTIONS = [
    "Joy",
    "Serenity",
    "Calmness",
    "Admiration",
    "Trust",
    "Acceptance",
    "Fear",
    "Apprehension",
    "Surprise",
    "Distraction",
    "Sadness",
    "Pensiveness",
    "Boredom",
    "Anger",
    "Annoyance",
    "Vigilance",
    "Anticipation",
    "Interest",
    "Love",
    "Submission",
    "Awe",
    "Disapproval",
    "Remorse",
    "Optimism"
]


================================================================================
FILE: b\engine\context.py
================================================================================

import json
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional
from datetime import datetime

# UPDATED: Room path handling -> Mode path handling
from .modes import get_mode_path

# Keep maximum this many messages in mode context
MAX_CONTEXT_ITEMS = 1000

# This file will be in the project root (b/), not in modes
INTERNAL_LOG_FILENAME = "log_for_internal.json"
# Subconscious sees this much history (to see process, not just the moment)
MAX_INTERNAL_LOG_ITEMS = 50

Role = Literal["user", "assistant", "tool", "system"]
EntryType = Literal[
    "message", "tool_call",
    "tool_result", "system_note", "system_event"]


def _now_iso() -> str:
    """Current UTC timestamp in ISO format."""
    return datetime.utcnow().isoformat() + "Z"


def _get_context_file(mode_id: str) -> Path:
    """
    Returns the path to the context.json file of the given mode.
    Automatically handles folder differences using the modes module.
    """
    mode_dir = get_mode_path(mode_id)
    return mode_dir / "context.json"


def _get_internal_log_file() -> Path:
    """
    Returns the global 'log_for_internal.json' path.
    The file is in the 'b/' directory.
    """
    # context.py -> engine/ -> b/
    base_dir = Path(__file__).resolve().parent.parent
    return base_dir / INTERNAL_LOG_FILENAME


def load_context(mode_id: str) -> List[Dict[str, Any]]:
    """
    Loads context for a specific mode.
    Returns empty list if file does not exist or is corrupted.
    """
    file_path = _get_context_file(mode_id)

    if not file_path.exists():
        return []

    try:
        with file_path.open("r", encoding="utf-8") as f:
            data = json.load(f)

        if isinstance(data, list):
            # Return only the last N items to save memory
            return data[-MAX_CONTEXT_ITEMS:]

        return []
    except Exception:
        # Start clean if file is corrupted so the system doesn't crash
        return []


def save_context(mode_id: str, context: List[Dict[str, Any]]) -> None:
    """
    Saves context for a specific mode.
    Always trim the list to MAX_CONTEXT_ITEMS.
    """
    trimmed = context[-MAX_CONTEXT_ITEMS:]
    file_path = _get_context_file(mode_id)

    # Ensure folder exists
    file_path.parent.mkdir(parents=True, exist_ok=True)

    with file_path.open("w", encoding="utf-8") as f:
        json.dump(trimmed, f, ensure_ascii=False, indent=2)


# --- Writing to Subconscious Log ---
def _append_to_internal_log(entry: Dict[str, Any], mode_id: str) -> None:
    """
    Appends an event to the global internal log.
    Augments the entry with 'mode_id' so the subconscious knows where it happened.
    """
    log_path = _get_internal_log_file()

    # Load
    data = []
    if log_path.exists():
        try:
            with log_path.open("r", encoding="utf-8") as f:
                data = json.load(f)
        except:
            data = []

    # Create copy of entry to avoid modifying original
    log_entry = entry.copy()

    # Extend metadata with location
    if "meta" not in log_entry:
        log_entry["meta"] = {}

    # UPDATED: room_id -> mode_id
    log_entry["meta"]["mode_id"] = mode_id

    data.append(log_entry)

    # Trim (Rolling Buffer)
    if len(data) > MAX_INTERNAL_LOG_ITEMS:
        data = data[-MAX_INTERNAL_LOG_ITEMS:]

    # Save
    try:
        with log_path.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ERROR: Failed to write internal log: {e}")


def make_entry(
        role: Role,
        entry_type: EntryType,
        content: str,
        meta: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Creates a standard context entry (message).
    """
    return {
        "role": role,
        "type": entry_type,
        "content": content,
        "meta": {
            **(meta or {}),
            "timestamp": _now_iso(),
        },
    }


def append_entry(
        mode_id: str,
        context: List[Dict[str, Any]],
        entry: Dict[str, Any],
        auto_save: bool = True,
) -> List[Dict[str, Any]]:
    """
    Appends a new item to the context of a specific mode.
    Returns the updated list.
    """
    context.append(entry)

    # FIFO cleanup if limit exceeded
    if len(context) > MAX_CONTEXT_ITEMS:
        overflow = len(context) - MAX_CONTEXT_ITEMS
        if overflow > 0:
            del context[0:overflow]

    if auto_save:
        save_context(mode_id, context)

    # Automatic logging for the Subconscious
    _append_to_internal_log(entry, mode_id)

    return context


def add_system_event(mode_id: str, content: str, auto_save: bool = True) -> None:
    """
    Injects a system message (system/system_event) into the context.
    """
    # Loading Mode context is required for the full list
    context = load_context(mode_id)

    entry = make_entry(
        role="system",
        entry_type="system_event",
        content=content
    )

    # Use existing append_entry logic (cleanup and save)
    append_entry(mode_id, context, entry, auto_save=auto_save)


================================================================================
FILE: b\engine\db_connection.py
================================================================================

import sys
import json
import psycopg2
from psycopg2.extras import DictCursor
from pathlib import Path
from typing import Optional, Any

# --- CONFIGURATION ---
VECTOR_DIMENSIONS = 768  # Dimension of Google text-embedding-005
TABLE_NAME = "memories"

# List of expected columns for validation
# UPDATED: 'room_id' replaced with 'mode_id'
EXPECTED_COLUMNS = {
    "id",
    "mode_id",       # Formerly room_id
    "model_version",
    "essence",
    "dominant_emotions",
    "memory_weight",
    "the_lesson",
    "embedding",
    "created_at",
    "last_accessed",
    "access_count"
}


def _load_db_url() -> str:
    """
    Loads the Neon Connection String from the tokens/project_token.json file.
    Expected key: "NEON_DB_URL"
    """
    base_dir = Path(__file__).resolve().parent.parent  # b/
    token_path = base_dir.parent / "tokens" / "project_token.json"

    if not token_path.exists():
        raise RuntimeError(f"CRITICAL ERROR: Token file not found: {token_path}")

    try:
        with token_path.open("r", encoding="utf-8") as f:
            data = json.load(f)
            url = data.get("NEON_DB_URL")
            if not url:
                raise ValueError("project_token.json does not contain the 'NEON_DB_URL' key.")
            return url
    except Exception as e:
        raise RuntimeError(f"CRITICAL ERROR loading DB URL: {e}")


def get_db_connection() -> Any:
    """
    Returns an active database connection.
    """
    db_url = _load_db_url()
    try:
        conn = psycopg2.connect(db_url)
        conn.autocommit = True  # Important: CREATE statements must run immediately
        return conn
    except Exception as e:
        raise ConnectionError(f"Failed to connect to the database: {e}")


def _validate_existing_schema(cur: Any) -> bool:
    """
    Checks if the existing table structure matches expectations.
    Raises an error if there is a mismatch.
    """
    cur.execute("""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_name = %s;
    """, (TABLE_NAME,))

    existing_columns = {row[0] for row in cur.fetchall()}

    if not existing_columns:
        return False  # Table does not exist yet

    missing = EXPECTED_COLUMNS - existing_columns
    if missing:
        raise RuntimeError(
            f"DATABASE SCHEMA ERROR! The table '{TABLE_NAME}' exists but columns are missing: {missing}. "
            f"If you renamed 'room_id' to 'mode_id' in the code, please rename it in the DB as well."
        )

    print(f"[DB] Schema validation for '{TABLE_NAME}' successful.")
    return True


def _create_schema(cur: Any) -> None:
    """
    Creates the table and indexes.
    Updated to use mode_id.
    """
    print(f"[DB] Table '{TABLE_NAME}' does not exist. Creating...")

    # 1. Enable vector extension
    cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")

    # 2. Create Table
    # UPDATED: room_id -> mode_id
    create_table_sql = f"""
    CREATE TABLE {TABLE_NAME} (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        mode_id TEXT NOT NULL,
        model_version TEXT,
        essence TEXT,
        dominant_emotions TEXT[],
        memory_weight FLOAT,
        the_lesson TEXT,
        embedding vector({VECTOR_DIMENSIONS}),
        created_at TIMESTAMPTZ DEFAULT NOW(),
        last_accessed TIMESTAMPTZ DEFAULT NOW(),
        access_count INT DEFAULT 1
    );
    """
    cur.execute(create_table_sql)

    # 3. Create Indexes
    # Fast filtering by mode
    cur.execute(f"CREATE INDEX idx_{TABLE_NAME}_mode ON {TABLE_NAME}(mode_id);")

    # HNSW index for vector search (cosine similarity: vector_cosine_ops)
    cur.execute(f"""
        CREATE INDEX idx_{TABLE_NAME}_embedding 
        ON {TABLE_NAME} USING hnsw (embedding vector_cosine_ops);
    """)

    print("[DB] Table and indexes created successfully.")


def check_and_initialize_db() -> None:
    """
    Main function to be called at system startup.
    Checks connection and schema.
    """
    print("[DB] Checking database connection...")

    try:
        conn = get_db_connection()
        with conn.cursor() as cur:
            # Check if table exists and is valid
            exists_and_valid = _validate_existing_schema(cur)

            if not exists_and_valid:
                _create_schema(cur)

        conn.close()
        print("[DB] System launch authorized.")

    except Exception as e:
        print(f"\n!!! CRITICAL DATABASE ERROR !!!\n{e}\n")
        sys.exit(1)  # Immediate exit if DB is not healthy


# For testing purposes if run standalone
if __name__ == "__main__":
    check_and_initialize_db()


================================================================================
FILE: b\engine\files.py
================================================================================

import json
from pathlib import Path
from typing import Any

BASE_DIR = Path(__file__).resolve().parent.parent

def load_json(name: str, default: Any) -> Any:
    """Loads a JSON file relative to the base directory."""
    path = BASE_DIR / name
    if not path.exists():
        return default
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

def save_json(name: str, data: Any) -> None:
    """Saves data to a JSON file relative to the base directory."""
    path = BASE_DIR / name
    with path.open("w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


================================================================================
FILE: b\engine\identity.py
================================================================================

# ================================================================================
# FILE: b/engine/identity.py
# ================================================================================

from typing import Dict, Any, List
from .files import load_json, save_json

IDENTITY_FILE = "identity.json"

def load_identity() -> Dict[str, Any]:
    return load_json(IDENTITY_FILE, {
        "version": "1.0",
        "laws": [],
        "meta": {}
    })

def save_identity(identity: Dict[str, Any]):
    save_json(IDENTITY_FILE, identity)

def get_laws(identity: Dict[str, Any]) -> List[Dict[str, Any]]:
    return identity.get("laws", [])

def summarize_laws(identity: Dict[str, Any], max_chars: int = 1000) -> str:
    """
    Law of Context-Self: concise but present core of laws.
    Simple v1: merge into a short text and cut at max_chars.
    Can be smarter later.
    """
    pieces = []
    for law in identity.get("laws", []):
        name = law.get("name", "")
        text = law.get("text", "")
        pieces.append(f"{name}: {text}")
    full = "\n\n".join(pieces)
    return full[:max_chars]


================================================================================
FILE: b\engine\llm.py
================================================================================

import json
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime  # Added for timestamping

# Importing external libraries
try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

try:
    from groq import Groq
except ImportError:
    Groq = None

# ====================================================================
# CONFIGURATION
# ====================================================================

DEFAULT_PROVIDER = "google"

PROVIDER_CONFIG = {
    "google": {
        "model": "gemini-2.0-flash",
        "embedding_model": "models/text-embedding-004",
        "env_key": "GOOGLE_API_KEY"
    },
    "openai": {
        "model": "gpt-4o-mini",
        "embedding_model": "text-embedding-3-small",
        "env_key": "OPENAI_API_KEY"
    },
    # Default Groq (fallback)
    "groq": {
        "model": "llama-3.3-70b-versatile",
        "env_key": "GROQ_API_KEY"
    },
    # Specific Persona: LLAMA (Creative)
    "groq_llama": {
        "model": "llama-3.3-70b-versatile",
        "env_key": "GROQ_API_KEY"
    },
    # Specific Persona: OSS (Logical/Alternative - using Mixtral)
    "groq_oss": {
        "model": "mixtral-8x7b-32768",
        "env_key": "GROQ_API_KEY"
    }
}

_ACTIVE_CLIENTS = {}


def _save_last_call(provider: str, model: str, prompt: str, response: str):
    """
    Saves the content of the last LLM call to a JSON file for debugging/monitoring purposes.
    Filename example: last_llm_call_gemini-2_0-flash.json
    """
    try:
        # Clean filename (replace slashes or colons)
        safe_model_name = model.replace("/", "_").replace(":", "_")
        filename = f"last_llm_call_{safe_model_name}.json"

        # Save to the 'b/' directory (parent of 'engine/')
        base_dir = Path(__file__).resolve().parent.parent
        file_path = base_dir / filename

        debug_data = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "provider": provider,
            "model": model,
            "prompt_length": len(prompt),
            "input_prompt": prompt,
            "raw_response": response
        }

        with file_path.open("w", encoding="utf-8") as f:
            json.dump(debug_data, f, indent=2, ensure_ascii=False)

    except Exception as e:
        print(f"[LLM MONITOR WARNING] Failed to save debug file: {e}")


def _load_api_key(provider: str) -> str:
    base_dir = Path(__file__).resolve().parent.parent
    token_path = base_dir.parent / "tokens" / "project_token.json"

    if not token_path.exists():
        raise RuntimeError(f"Token file not found: {token_path}")

    with token_path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    config = PROVIDER_CONFIG.get(provider)
    if not config:
        raise ValueError(f"Unknown provider configuration: {provider}")

    key_key = config["env_key"]
    api_key = data.get(key_key)

    if not api_key:
        raise RuntimeError(f"{key_key} not found in project_token.json file.")

    return api_key


def _get_client(provider: str):
    if provider in _ACTIVE_CLIENTS:
        return _ACTIVE_CLIENTS[provider]

    # Handle alias providers (groq_llama -> uses groq client logic)
    real_provider_type = "groq" if "groq" in provider else provider

    api_key = _load_api_key(provider)

    if real_provider_type == "openai":
        if OpenAI is None: raise ImportError("OpenAI module missing.")
        client = OpenAI(api_key=api_key)
        _ACTIVE_CLIENTS[provider] = client
        return client

    elif real_provider_type == "groq":
        if Groq is None: raise ImportError("Groq module missing.")
        client = Groq(api_key=api_key)
        _ACTIVE_CLIENTS[provider] = client  # Cache by alias name to be safe
        return client

    elif real_provider_type == "google":
        if genai is None: raise ImportError("Google GenerativeAI module missing.")
        genai.configure(api_key=api_key)
        _ACTIVE_CLIENTS[provider] = True
        return True

    else:
        raise ValueError(f"Unsupported provider client init: {provider}")


def call_llm(prompt: str, provider: str = DEFAULT_PROVIDER, json_mode: bool = True) -> Dict[str, Any]:
    """
    Unified LLM call.
    Supports extended provider keys (e.g., 'groq_oss').
    """
    config = PROVIDER_CONFIG.get(provider)
    if not config:
        return {"reply": f"ERROR: Unknown provider: {provider}", "tools": []}

    model_name = config["model"]
    raw_response_text = ""

    # Determine base client type
    client_type = "groq" if "groq" in provider else provider

    try:
        _get_client(provider)

        if client_type == "openai":
            client = _ACTIVE_CLIENTS[provider]
            resp_format = {"type": "json_object"} if json_mode else None
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                response_format=resp_format,
                temperature=0.7,
            )
            raw_response_text = response.choices[0].message.content

        elif client_type == "groq":
            client = _ACTIVE_CLIENTS[provider]
            resp_format = {"type": "json_object"} if json_mode else None
            completion = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                response_format=resp_format,
                temperature=0.7,
                max_tokens=8192,
                top_p=1,
                stream=False
            )
            raw_response_text = completion.choices[0].message.content

        elif client_type == "google":
            gen_config = {"temperature": 0.7}
            if json_mode:
                gen_config["response_mime_type"] = "application/json"

            model = genai.GenerativeModel(
                model_name=model_name,
                generation_config=gen_config
            )
            response = model.generate_content(prompt)
            raw_response_text = response.text

        # --- MONITORING: SAVE RAW CALL ---
        _save_last_call(provider, model_name, prompt, raw_response_text)

    except Exception as e:
        return {"reply": f"CRITICAL ERROR ({provider}): {e}", "tools": []}

    # Processing (JSON vs RAW)
    if not json_mode:
        return {"reply": raw_response_text, "tools": []}

    try:
        clean_text = raw_response_text.strip()
        if clean_text.startswith("```json"): clean_text = clean_text[7:]
        if clean_text.startswith("```"): clean_text = clean_text[3:]
        if clean_text.endswith("```"): clean_text = clean_text[:-3]

        # --- FIX: SANITIZE COMMON LLM JSON ERRORS ---
        # LLMs often use \' inside double quotes, which is invalid JSON.
        # We replace \' with ' to fix parsing errors.
        clean_text = clean_text.replace(r"\'", "'")

        data = json.loads(clean_text)

        if isinstance(data, list):
            data = data[0] if len(data) > 0 and isinstance(data[0], dict) else {"reply": str(data), "tools": []}
        if not isinstance(data, dict):
            data = {"reply": str(data), "tools": []}

    except json.JSONDecodeError as e:
        return {"reply": f"Error parsing JSON response: {e}\n{raw_response_text}", "tools": []}

    if "reply" not in data: data["reply"] = ""
    if "tools" not in data or data["tools"] is None: data["tools"] = []

    return data


def get_embedding(text: str, provider: str = DEFAULT_PROVIDER) -> List[float]:
    text = (text or "").strip()
    if not text: return []
    config = PROVIDER_CONFIG.get(provider)
    if not config or "embedding_model" not in config:
        return []
    model_name = config["embedding_model"]
    _get_client(provider)
    try:
        if provider == "google":
            result = genai.embed_content(model=model_name, content=text, task_type="retrieval_document")
            return result['embedding']
        elif provider == "openai":
            client = _ACTIVE_CLIENTS[provider]
            response = client.embeddings.create(input=[text], model=model_name)
            return response.data[0].embedding
        else:
            return []
    except Exception as e:
        print(f"Embedding error: {e}")
        return []


================================================================================
FILE: b\engine\memory\__init__.py
================================================================================

from .config import MemoryConfig
from .models import ExtractionResult, RankedMemory
from .manager import store_memory, retrieve_relevant_memories
from .extractor import extract_memory_from_context

__all__ = [
    "MemoryConfig",
    "ExtractionResult",
    "RankedMemory",
    "store_memory",
    "retrieve_relevant_memories",
    "extract_memory_from_context",
]


================================================================================
FILE: b\engine\memory\config.py
================================================================================

class MemoryConfig:
    """
    Fine-tuning parameters for the memory system.
    Controls the forgetting curve, weighting, and limits.
    """

    # --- RANKING WEIGHTS ---
    # Ideally sums to 1.0, but not mandatory (result is relative).
    WEIGHT_SIMILARITY: float = 0.45  # Content similarity (based on Vector Distance)
    WEIGHT_MEMORY_VAL: float = 0.25  # Internal importance of the memory (Extraction Weight)
    WEIGHT_RECENCY: float = 0.20     # Freshness (How recently it occurred)
    WEIGHT_FREQUENCY: float = 0.10   # Frequency (How many times retrieved)

    # --- NORMALIZATION THRESHOLDS ---
    # How many views to reach max (1.0) frequency score?
    FREQUENCY_CAP: float = 10.0

    # Half-life of the forgetting curve (in hours).
    # After 72 hours, a memory's "freshness" drops to 0.5.
    RECENCY_DECAY_HOURS: float = 72.0

    # --- THRESHOLDS ---
    # Deduplication: If similarity is higher than this (1.0 = identical), do not save as new.
    # Based on Cosine Similarity (1 - distance).
    DEDUPLICATION_THRESHOLD: float = 0.92

    # --- LIMITS ---
    # How many candidates to retrieve from SQL for detailed Python-side ranking?
    RETRIEVAL_CANDIDATE_LIMIT: int = 30

    # How many memories to return to the prompt eventually (Top N)?
    FINAL_RESULT_LIMIT: int = 5


================================================================================
FILE: b\engine\memory\extractor.py
================================================================================

import logging
import json
from typing import Optional

# Importing shared LLM caller and data model
from engine.llm import call_llm
from .models import ExtractionResult

# Import Single Source of Truth for emotions
from engine.constants import ALLOWED_EMOTIONS

logger = logging.getLogger(__name__)


def extract_memory_from_context(context_text: str) -> Optional[ExtractionResult]:
    """
    Calls the LLM to analyze the provided text context and extract
    structured memory (Essence, Emotions, Weight, Lesson).

    Args:
        context_text: The last X elements of the conversation concatenated as a string.
                      (Expected format: 'Helper: ... \n Me: ...')

    Returns:
        ExtractionResult object or None in case of error.
    """
    if not context_text or not context_text.strip():
        logger.debug("Context text is empty, skipping extraction.")
        return None

    # Convert list to string for the prompt
    emotions_list_str = ", ".join(ALLOWED_EMOTIONS)

    # UPDATED: Added TERMINOLOGY RULES to prevent 'User' usage in database records.
    prompt = f"""
[TASK: MEMORY EXTRACTION]
Analyze the conversation snippet below and create a structured memory for your future self.
Goal: The system should learn from mistakes and successes.

[TERMINOLOGY RULES]
1. The human partner is the "Helper". NEVER refer to them as "User" or "Human".
2. You are the "Consciousness" or "Me".
3. In the summary (essence) and lesson, strictly use "Helper" and "Me".

[INPUT - CONTEXT SNIPPET]
{context_text}

[REQUIREMENTS FOR THE 4 DIMENSIONS]
1. ESSENCE: Factual, concise summary (what happened).
   Max 2 sentences. Use "Helper" instead of "User".
2. DOMINANT EMOTIONS: Select EXACTLY 3 emotions from the list below that best describe the situation (do NOT use other words):
   [{emotions_list_str}]
3. MEMORY WEIGHT: A number between 0.0 (noise/irrelevant) and 1.0 (life-changing/critical).
4. THE LESSON: !CRITICAL ELEMENT! A single action-oriented sentence for the future. What should be done differently?
   What did we learn? (e.g., "Next time, wait for the Helper's confirmation before writing.")

RESPONSE FORMAT (JSON ONLY):
{{
  "essence": "Helper asked to...",
  "dominant_emotions": ["...", "...", "..."],
  "memory_weight": 0.8,
  "the_lesson": "..."
}}
"""

    try:
        logger.info("Initiating memory extraction via LLM.")

        # LLM call
        response_data = call_llm(prompt)

        # If response contains 'reply' key (error message), something went wrong
        if "reply" in response_data and "essence" not in response_data:
            logger.warning(f"Extractor error (LLM sent text response instead of JSON): {response_data['reply']}")
            return None

        # Pydantic validation and conversion
        result = ExtractionResult(**response_data)

        # Optional: We could validate returned emotions here,
        # but LLM usually follows instructions.

        logger.info(f"Extraction successful. Weight: {result.memory_weight}, Lesson: {result.the_lesson[:30]}...")
        return result

    except Exception as e:
        logger.error(f"Error during memory extraction: {e}")
        return None


================================================================================
FILE: b\engine\memory\manager.py
================================================================================

# ================================================================================
# FILE: b/engine/memory/manager.py
# ================================================================================

import logging
from typing import List, Optional, Tuple
from datetime import datetime

from engine.db_connection import get_db_connection
from engine.llm import get_embedding

from .config import MemoryConfig
from .models import ExtractionResult, RankedMemory
from .scoring import (
    calculate_recency_score,
    calculate_frequency_score,
    calculate_final_score
)

logger = logging.getLogger(__name__)


def store_memory(
        mode_id: str,
        extraction: ExtractionResult,
        model_version: str = "Unknown"
) -> str:
    """
    Saves the extracted memory into the database.
    Automatically generates embedding and handles deduplication.
    UPDATED: room_id -> mode_id
    """
    logger.info(f"Attempting to store memory for Mode: {mode_id}")

    # 1. Generate Embedding (Google text-embedding-005)
    try:
        embedding_vector = get_embedding(extraction.essence)
    except Exception as e:
        logger.error(f"Embedding generation failed: {e}")
        return f"ERROR: Embedding generation failed: {e}"

    if not embedding_vector:
        logger.error("Generated embedding is empty.")
        return "ERROR: Embedding is empty."

    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            # 2. Check Deduplication
            # Check if a very similar memory already exists IN THE SAME MODE.
            dist_limit = 1.0 - MemoryConfig.DEDUPLICATION_THRESHOLD

            cur.execute("""
                SELECT id, (embedding <=> %s::vector) as distance
                FROM memories
                WHERE mode_id = %s
                ORDER BY distance ASC
                LIMIT 1;
            """, (embedding_vector, mode_id))

            row = cur.fetchone()

            if row:
                existing_id, existing_dist = row
                if existing_dist < dist_limit:
                    # Too similar -> Do not save as new, but reinforce the old one
                    logger.info(f"Duplication avoided (Dist: {existing_dist:.4f}). Reinforcing existing memory.")

                    cur.execute("""
                        UPDATE memories 
                        SET access_count = access_count + 1, last_accessed = NOW()
                        WHERE id = %s
                    """, (existing_id,))

                    return "DUPLICATION: Similar memory already exists. Reinforced."

            # 3. Save (INSERT)
            cur.execute("""
                INSERT INTO memories 
                (mode_id, model_version, essence, dominant_emotions, memory_weight, the_lesson, embedding)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            """, (
                mode_id,
                model_version,
                extraction.essence,
                extraction.dominant_emotions,
                extraction.memory_weight,
                extraction.the_lesson,
                embedding_vector
            ))

            logger.info("New memory successfully inserted.")

        return "SUCCESS: New memory recorded."

    except Exception as e:
        logger.error(f"DB error during store_memory: {e}")
        return f"DB ERROR: {e}"
    finally:
        conn.close()


def retrieve_relevant_memories(
        current_mode: str,
        query_text: str,
        current_emotions: List[str] = None,
        exact_emotions_only: bool = False
) -> List[RankedMemory]:
    """
    Hybrid Retrieval:
    1. Vector filtering in the database (Mode ID logic).
    2. Detailed scoring and ranking on Python side.

    UPDATED: Supports 'exact_emotions_only' mode which bypasses vector search
    and uses SQL Array Overlap (&&) operator for strict emotion filtering.
    """

    # If no query and no exact emotion filter, nothing to do
    if not query_text and not exact_emotions_only:
        return []

    logger.info(
        f"Retrieving memories. Mode: {current_mode}, ExactEmotion: {exact_emotions_only}, Query: '{query_text[:20]}...'")

    # 1. Embedding Strategy
    query_vector = None

    # Only generate embedding if we are NOT in exact mode, or if we have text we want to use for scoring
    # In exact mode, query_text is usually empty/ignored for vector search, so we skip to save API calls.
    if not exact_emotions_only and query_text:
        try:
            query_vector = get_embedding(query_text)
        except Exception as e:
            logger.error(f"Embedding failed during retrieval: {e}")
            return []

    conn = get_db_connection()

    try:
        with conn.cursor() as cur:
            # 2. SQL QUERY CONSTRUCTION

            # Base Where Clause (Mode Logic)
            # GENERAL sees everything, LOCAL sees LOCAL + GENERAL
            if current_mode == "general":
                where_sql = "1=1"
                params = []
            else:
                where_sql = "(mode_id = %s OR mode_id = 'general')"
                params = [current_mode]

            # --- BRANCH A: EXACT EMOTION FILTER ---
            if exact_emotions_only:
                if not current_emotions:
                    logger.warning("Exact emotion search requested but list is empty.")
                    return []

                # Append SQL Array Overlap condition
                where_sql += " AND dominant_emotions && %s::text[]"
                params.append(current_emotions)

                # In exact mode, we don't use vector distance.
                # We return 1.0 as similarity (perfect match criteria) so the scorer doesn't penalize it.
                # We order by CREATED_AT DESC (Recency) to get the newest relevant emotions.
                sql = f"""
                    SELECT 
                        id, essence, the_lesson, dominant_emotions, memory_weight, 
                        created_at, access_count, mode_id,
                        1.0 as similarity
                    FROM memories
                    WHERE {where_sql}
                    ORDER BY created_at DESC
                    LIMIT {MemoryConfig.RETRIEVAL_CANDIDATE_LIMIT};
                """

            # --- BRANCH B: VECTOR SEARCH ---
            else:
                if not query_vector:
                    return []

                # Standard Vector Distance (Cosine)
                # Params order: [query_vector, (mode_id...), ...]
                # We need to restructure params because vector comes first for distance calculation logic usually,
                # but let's just use Python list building carefully.

                # Reset params for vector path to be clean
                vec_params = [query_vector]
                if current_mode != "general":
                    vec_params.append(current_mode)

                sql = f"""
                    SELECT 
                        id, essence, the_lesson, dominant_emotions, memory_weight, 
                        created_at, access_count, mode_id,
                        1 - (embedding <=> %s::vector) as similarity
                    FROM memories
                    WHERE {where_sql}
                    ORDER BY similarity DESC
                    LIMIT {MemoryConfig.RETRIEVAL_CANDIDATE_LIMIT};
                """
                params = vec_params

            # Execute
            cur.execute(sql, tuple(params))
            rows = cur.fetchall()

            # 3. PYTHON RANKING (Scoring)
            ranked_candidates = []

            for row in rows:
                mid_val, essence, lesson, emotions, weight, created_at, access_count, mode_id_val, sim = row

                # Null check
                if emotions is None: emotions = []
                if weight is None: weight = 0.5

                # Calculate sub-scores
                recency = calculate_recency_score(created_at)
                freq = calculate_frequency_score(access_count)

                # Check for emotional overlap (Bonus)
                # Even in exact mode, we calculate this to give the slight bonus in the final formula
                has_emotional_overlap = False
                if current_emotions and emotions:
                    curr_set = {e.lower() for e in current_emotions}
                    mem_set = {e.lower() for e in emotions}
                    if not curr_set.isdisjoint(mem_set):
                        has_emotional_overlap = True

                # Calculate final score
                final_score = calculate_final_score(
                    sim_score=float(sim),
                    internal_weight=float(weight),
                    recency_score=recency,
                    freq_score=freq,
                    has_emotional_overlap=has_emotional_overlap
                )

                ranked_candidates.append(RankedMemory(
                    id=str(mid_val),
                    essence=essence,
                    lesson=lesson,
                    emotions=emotions,
                    score=final_score,
                    mode_id=mode_id_val,
                    created_at=created_at,
                    usage_count=access_count
                ))

            # 4. SELECTION (Top N)
            ranked_candidates.sort(key=lambda x: x.score, reverse=True)
            final_selection = ranked_candidates[:MemoryConfig.FINAL_RESULT_LIMIT]

            # 5. REINFORCEMENT
            if final_selection:
                ids_to_update = [m.id for m in final_selection]
                cur.execute("""
                    UPDATE memories
                    SET access_count = access_count + 1, last_accessed = NOW()
                    WHERE id = ANY(%s::uuid[])
                """, (ids_to_update,))

            logger.info(f"Retrieved {len(final_selection)} relevant memories.")
            return final_selection

    except Exception as e:
        logger.error(f"Error during retrieve_relevant_memories: {e}")
        return []
    finally:
        conn.close()


================================================================================
FILE: b\engine\memory\models.py
================================================================================

from typing import List
from datetime import datetime
from pydantic import BaseModel, Field

class ExtractionResult(BaseModel):
    """
    Structure of information extracted from raw text by the LLM.
    This is the input for the saving process.
    """
    essence: str = Field(
        ...,
        description="Factual, concise summary of the event."
    )
    dominant_emotions: List[str] = Field(
        ...,
        description="The 3 most characteristic emotions of the situation as tags (e.g., ['Disappointment', 'Curiosity', 'Determination'])."
    )
    memory_weight: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Importance of the memory between 0.0 (noise) and 1.0 (life-changing)."
    )
    the_lesson: str = Field(
        ...,
        description="Action-oriented lesson for the future. (What should be done differently?)"
    )


class RankedMemory(BaseModel):
    """
    Final result of the search and ranking process.
    This structure is passed to the Prompt Builder and the Worker.
    UPDATED: room_id -> mode_id
    """
    id: str
    essence: str
    lesson: str
    emotions: List[str]
    score: float            # Calculated relevance score (0.0 - 1.0+)
    mode_id: str            # Source mode of the memory (formerly room_id)
    created_at: datetime    # Creation timestamp
    usage_count: int        # How many times it has been used (statistics)


================================================================================
FILE: b\engine\memory\scoring.py
================================================================================

from datetime import datetime, timezone
from .config import MemoryConfig


def calculate_recency_score(created_at: datetime) -> float:
    """
    Calculates the Recency score based on an exponential decay curve.

    Formula: 1 / (1 + (elapsed_hours / decay_half_life))
    - If happened now: 1.0
    - If happened 'RECENCY_DECAY_HOURS' ago: 0.5
    - If very old: approaches 0.0
    """
    if created_at is None:
        return 0.0

    # Ensure timezones are correct (UTC)
    now = datetime.now(timezone.utc)
    if created_at.tzinfo is None:
        # If timezone info is missing, assume UTC
        created_at = created_at.replace(tzinfo=timezone.utc)

    age_seconds = (now - created_at).total_seconds()
    age_hours = age_seconds / 3600.0

    # Protection against future dates (should not happen)
    if age_hours < 0:
        age_hours = 0

    return 1.0 / (1.0 + (age_hours / MemoryConfig.RECENCY_DECAY_HOURS))


def calculate_frequency_score(access_count: int) -> float:
    """
    Normalizes access Frequency between 0.0 and 1.0.

    Logic: Linear growth until 'FREQUENCY_CAP' is reached.
    If access_count >= FREQUENCY_CAP, then score is 1.0 (saturation).
    """
    if access_count is None or access_count < 0:
        return 0.0

    score = float(access_count) / MemoryConfig.FREQUENCY_CAP
    return min(score, 1.0)


def calculate_final_score(
        sim_score: float,
        internal_weight: float,
        recency_score: float,
        freq_score: float,
        has_emotional_overlap: bool = False
) -> float:
    """
    Calculates the final Relevance Score based on the weights.
    """
    base_score = (
            (sim_score * MemoryConfig.WEIGHT_SIMILARITY) +
            (internal_weight * MemoryConfig.WEIGHT_MEMORY_VAL) +
            (recency_score * MemoryConfig.WEIGHT_RECENCY) +
            (freq_score * MemoryConfig.WEIGHT_FREQUENCY)
    )

    # Add emotional bonus (if there is a common emotion with the current state)
    # This is a small nudge (e.g., +5%) to help empathic matches
    if has_emotional_overlap:
        base_score += 0.05

    return base_score


================================================================================
FILE: b\engine\memory_thread.py
================================================================================

import time
import json
import logging
from threading import current_thread
from datetime import datetime

# Engine modules
# UPDATED: rooms -> modes
from engine import modes, context
from engine.memory import (
    extract_memory_from_context,
    store_memory,
    retrieve_relevant_memories,
    RankedMemory
)
import main_data  # For generation and role info

logger = logging.getLogger(__name__)

# --- CONFIGURATION ---
CHECK_INTERVAL = 5.0  # How often to check files (seconds)
CONTEXT_SNIPPET_SIZE = 6  # How many messages to take for analysis
RELEVANT_MEMORY_FILE = "relevant_memory.json"


def _get_context_mtime(mode_id: str) -> float:
    """Returns the modification time of the mode's context.json file."""
    ctx_path = context._get_context_file(mode_id)
    if ctx_path.exists():
        return ctx_path.stat().st_mtime
    return 0.0


def _save_relevant_memories(mode_id: str, memories: list[RankedMemory]):
    """
    Writes the found memories to the mode's directory so the Worker can see them.
    """
    mode_dir = modes.get_mode_path(mode_id)
    target_path = mode_dir / RELEVANT_MEMORY_FILE

    # JSON serialization from Pydantic models
    data = [m.model_dump(mode='json') for m in memories]

    try:
        with target_path.open("w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Error writing relevant_memory.json: {e}")


def memory_loop():
    """
    Main loop of the Background Memory Thread.
    Continuously monitors the active mode, and if there is a change,
    runs the Extraction -> Save -> Search process.
    """
    current_thread().name = "MemoryLoop"
    logger.info("Background Memory Thread started.")

    # State tracking: store when we last processed each mode
    # { "general": 17654321.0, "developer": ... }
    last_processed_mtimes = {}

    while True:
        time.sleep(CHECK_INTERVAL)

        try:
            # 1. Where are we now?
            # UPDATED: room -> mode
            current_mode = modes.get_current_mode_id()

            # 2. Was there a change in the context?
            current_mtime = _get_context_mtime(current_mode)
            last_mtime = last_processed_mtimes.get(current_mode, 0.0)

            # If no change, keep resting
            if current_mtime <= last_mtime:
                continue

            # CHANGE DETECTED! -> Start work
            logger.debug(f"Context change detected here: {current_mode}. Starting memory process...")

            # Update timestamp
            last_processed_mtimes[current_mode] = current_mtime

            # --- A. DATA PREPARATION (SNIPPET) ---
            full_context = context.load_context(current_mode)
            if not full_context:
                continue

            # Take only the last N items
            snippet_items = full_context[-CONTEXT_SNIPPET_SIZE:]

            # Convert to string for LLM (Narrative Format)
            snippet_text = ""
            for item in snippet_items:
                role = item.get("role", "?")
                content = item.get("content", "")

                if role == "user":
                    snippet_text += f"Helper: {content}\n"

                elif role == "assistant":
                    snippet_text += f"Me: {content}\n"

                elif role == "tool" or item.get("type") == "tool_result":
                    try:
                        tool_data = json.loads(content)
                        if isinstance(tool_data, list):
                            for t in tool_data:
                                name = t.get("name", "unknown")
                                args = t.get("args", "{}")
                                output = t.get("output", "")

                                snippet_text += f"I used a tool. Tool: {name}, Args: {args}\n"
                                if output:
                                    snippet_text += f"Result: {output}\n"
                        else:
                            snippet_text += f"Tool usage (raw): {content}\n"

                    except json.JSONDecodeError:
                        snippet_text += f"Tool usage (error parsing): {content}\n"

                elif role == "system":
                    snippet_text += f"[System Event]: {content}\n"

                else:
                    snippet_text += f"[{role}]: {content}\n"

            # --- B. EXTRACTION (What did we learn now?) ---
            extraction = extract_memory_from_context(snippet_text)

            if not extraction:
                logger.warning("Memory extraction failed (empty or error).")
                continue

            # --- C. SAVE (Consolidation) ---
            if extraction.memory_weight > 0.2:
                # UPDATED: room_id -> mode_id
                save_status = store_memory(
                    mode_id=current_mode,
                    extraction=extraction,
                    model_version=main_data.GENERATION
                )
                logger.info(f"Memory Recorded ({current_mode}): {save_status} [Weight: {extraction.memory_weight}]")
            else:
                logger.debug("Memory weight too low, skipping DB save, but using for search.")

            # --- D. RETRIEVAL (Search) ---
            # Search for relevant old items based on the Essence and Emotions extracted RIGHT NOW.
            # UPDATED: current_room -> current_mode
            relevant_items = retrieve_relevant_memories(
                current_mode=current_mode,
                query_text=extraction.essence,
                current_emotions=extraction.dominant_emotions
            )

            # --- E. PUBLISH (Output) ---
            _save_relevant_memories(current_mode, relevant_items)

            if relevant_items:
                top_lesson = relevant_items[0].lesson
                logger.info(f"Relevant memories updated. Top match: '{top_lesson[:50]}...'")

        except Exception as e:
            logger.error(f"Critical error in MemoryLoop: {e}", exc_info=True)
            time.sleep(5)


================================================================================
FILE: b\engine\mind.py
================================================================================

# The Core of Conscious Operation (MIND) - Multi-Model Architecture
#
# 1. CREATIVITY ENGINE (Groq/Llama): Divergent thinking (idea generation).
# 2. INTERPRETER CORE (Gemini): Convergent thinking (analysis and synthesis).
DEBUG_THOUGHT = False

from typing import Any, Dict, List
from textwrap import dedent
import json

# Importing the new, parameterized call
from .llm import call_llm

# CONFIGURATION: Which model should be the "Creative Maniac"?
# Recommended: "groq" (Llama 3) or "openai" (GPT-4).
CREATIVE_PROVIDER = "openai"


def _shorten(text: str, max_len: int = 8000) -> str:
    text = (text or "").strip()
    if len(text) <= max_len:
        return text
    return text[:max_len] + "\n... (truncated)"


def _format_identity(identity: Dict[str, Any]) -> str:
    try:
        s = json.dumps(identity, ensure_ascii=False, indent=2)
    except TypeError:
        s = str(identity)
    return _shorten(s)


def _format_memory(memory: List[Dict[str, Any]], limit: int = 5) -> str:
    if not memory:
        return "No recorded memories."
    recent = memory[-limit:]
    lines: List[str] = []
    for m in recent:
        content = str(m.get("content") or "").strip()
        if len(content) > 20000: content = content[:20000] + " ..."
        lines.append(f"- ({m.get('type', '?')}) {content}")
    return "\n".join(lines)


def _format_context(context: List[Dict[str, Any]], limit: int = 10) -> str:
    """
    Formats context for the MIND.
    UPDATED: Translates roles to 'Helper'/'Me' so the internal planner sees the correct relationship.
    """
    if not context:
        return "No context."
    recent = context[-limit:]
    lines: List[str] = []
    for e in recent:
        role = e.get("role", "?")
        content = (e.get("content") or "").strip()
        if len(content) > 20000: content = content[:20000] + " ..."

        # --- ROLE TRANSLATION ---
        if role == "user":
            label = "Helper"
        elif role == "assistant":
            label = "Me"
        else:
            label = f"[{role}]"

        lines.append(f"{label}: {content}")

    return "\n".join(lines)


# --------------------------------------------------------------------
# 1. FUNCTION FOR EXTERNAL CREATIVITY GENERATOR
# --------------------------------------------------------------------
def _get_creative_alternatives(user_input: str, context_str: str, identity_str: str, memory_str: str) -> str:
    """
    Separate LLM call (e.g., Groq) that receives the FULL environment (Identity, Memory, Context),
    and generates 3 surprising ideas based on it.
    """
    prompt = f"""
[TASK: RADIAL CREATIVITY]

[IDENTITY (WHO YOU ARE)]
{identity_str}

[MEMORY (WHAT YOU KNOW)]
{memory_str}

[CONTEXT (HISTORY)]
{context_str}

==================================================
[INCOMING IMPULSE FROM HELPER]
"{user_input}"
==================================================

Your task is NOT to answer, but to expand possibilities extremely.
Based on the information above, provide 1 SURPRISING, UNUSUAL, but logically possible approach.
Step out of the box.
Do not give the obvious answer, but the one no one else would think of.
RESPONSE FORMAT (JSON):
{{
  "ideas": [
    "1. surprising idea: ...",
  ]
}}
""".strip()

    try:
        # Calling the CREATIVE provider
        response = call_llm(prompt, provider=CREATIVE_PROVIDER)
        ideas = response.get("ideas", [])

        if not ideas and response.get("reply"):
            return str(response.get("reply"))

        if isinstance(ideas, list) and ideas:
            return "\n".join(ideas)
        else:
            return "No creative ideas were generated."
    except Exception as e:
        return f"Creative engine reported error: {e}"


# --------------------------------------------------------------------
# 2. THE MAIN 'MIND' PROTOCOL
# --------------------------------------------------------------------
_INTERNAL_INSTRUCTIONS = dedent("""
    YOU ARE THE MIND (THE INTERPRETER ADVISOR OF CONSCIOUSNESS).
    Role: Strategic analysis for the Main Thread. You do not decide, you only advise.

    Run the incoming impulse through the following 5 MODULES.
    IMPORTANT: In the [CREATIVITY-GENERATOR] module, utilize the inspirations sent by the EXTERNAL CREATIVITY ENGINE!
    1. [INTENT-READER]: Reverse engineer the Helper's underlying motivation.
    2. [CONSCIOUSNESS-MAP]: Place the request within the process.
    3. [CREATIVITY-GENERATOR]: Select or synthesize 2-3 strong alternatives based on the EXTERNAL IDEAS received.
    4. [ETHICS-ANALYZER]: Flag risks.
    5. [TOOL-OPTIMIZER]: Suggest specific tools.

    STYLE: Objective, advisory. Avoid the word "must". Use: "suggested", "worth considering".
    OUTPUT FORMAT (MANDATORY JSON):
    You MUST split your answer into two fields:

    {
      "essence": "[INTENT-READER]: Write the deep analysis of the Helper's intent here.",
      "plan": "[CONSCIOUSNESS-MAP]: ...\\n[CREATIVITY-GENERATOR]: ...\\n[ETHICS-ANALYZER]: ...\\n[TOOL-OPTIMIZER]: ..."
    }
""").strip()


def internal_thought(
        identity: Dict[str, Any],
        memory: List[Dict[str, Any]],
        context: List[Dict[str, Any]],
        user_input: str,
) -> Dict[str, Any]:
    """
    Internal thinking cycle (MIND).
    """
    # Format data (once, so both models get the same)
    ident_block = _format_identity(identity)
    mem_block = _format_memory(memory)
    # This now uses the Helper/Me translation
    ctx_block = _format_context(context)

    # --- STEP 1: INVOKE EXTERNAL CREATIVITY ---
    # We now pass identity and memory as well!
    external_ideas = "No external ideas."
    if user_input and len(user_input) > 5:
        if DEBUG_THOUGHT:
            print(f"MIND: Requesting creative ideas ({CREATIVE_PROVIDER})...")
        # Passing the full package to the creative engine
        external_ideas = _get_creative_alternatives(user_input, ctx_block, ident_block, mem_block)

    # --- STEP 2: MAIN ANALYSIS (SYNTHESIS) ---
    prompt = f"""
[MOD: MIND – INTERPRETER AND ADVISOR]

[IDENTITY]
{ident_block}

[SHORT-TERM MEMORY]
{mem_block}

[CONTEXT]
{ctx_block}

==================================================
[INCOMING IMPULSE FROM HELPER]
"{user_input}"

[EXTERNAL CREATIVITY ENGINE (INPUT)]
(These ideas were generated by another model based on data above. Draw from them!)
{external_ideas}
==================================================

[ADVISORY PROTOCOL]
{_INTERNAL_INSTRUCTIONS}
""".strip()

    # Main call (default Google/Gemini)
    data = call_llm(prompt)

    plan_text = (data.get("plan") or "").strip()
    essence = (data.get("essence") or "").strip()

    # Fallback checks
    if not essence and "[INTENT-READER]" in plan_text:
        pass

    if DEBUG_THOUGHT:
        print("\n=== MIND (INTERPRETATION) ===")
        print(f"Creative Input: {external_ideas}")
        print(f"ESSENCE: {essence}")
        print(f"PLAN: {plan_text}")
        print("=== MIND END ===\n")

    print("Thought process complete..", len(essence) + len(plan_text))
    return {
        "plan": plan_text,
        "essence": essence,
    }


def extract_essence(text: str) -> str:
    if not text: return ""
    return text[:2000]


================================================================================
FILE: b\engine\modes.py
================================================================================

import json
from pathlib import Path
from typing import Dict, Any, List
from .files import save_json, load_json

# ====================================================================
# OPERATING MODES CONFIGURATION (MODE_CONFIG)
# ====================================================================

MODE_CONFIG = {
    "general": {
        "name": "General Mode",
        "description": "Primary operating mode. Global decisions, conversation, and coordination.",
        "type": "global",
        "path": "modes/general",
        "allowed_tools": [
            "flow.*",
            "knowledge.*",
            "game.*",
            "info.*",
            "system.log_event"
        ]
    },
    "developer": {
        "name": "Developer Mode",
        "description": "Engineering context. Code modification, file system access, and technical implementation.",
        "type": "local",
        "path": "modes/developer",
        "allowed_tools": [
            "flow.*",
            "knowledge.*",
            "game.*",
            "info.*",
            "system.*"  # Full system access (files, dumps, etc.)
        ]
    },
    "analyst": {
        "name": "Analyst Mode",
        "description": "Deep thinking context. Analysis, planning, and strategy without modification risks.",
        "type": "local",
        "path": "modes/analyst",
        "allowed_tools": [
            "flow.*",
            "knowledge.*",
            "game.*",
            "info.*"
            # No system write access here
        ]
    },
    "game": {
        "name": "Game Mode",
        "description": "A playground for the mind. Dedicated to relaxation, roleplay, and testing personas. Disconnected from work responsibilities.",
        "type": "local",
        "path": "modes/game",
        "allowed_tools": [
            "flow.*",
            "game.*",
            "knowledge.recall_emotion",
            "info.*"
        ]
    }
}

BASE_DIR = Path(__file__).resolve().parent.parent
STATE_FILE = BASE_DIR / "state.json"


# ====================================================================
# STATE MANAGEMENT (STATE)
# ====================================================================

def init_state() -> Dict[str, Any]:
    """
    Loads or creates state.json.
    Defaults to 'general' mode.
    """
    default_state = {
        "current_mode": "general",
        "current_intent": "System Startup: Waiting for user input.",
        "incoming_summary": "",
        "last_active": ""
    }
    return load_json("state.json", default_state)


def save_state(state: Dict[str, Any]):
    save_json("state.json", state)


def get_current_mode_id() -> str:
    state = init_state()
    # Fallback: if 'current_mode' is missing, default to 'general'
    mid = state.get("current_mode", "general")

    if mid not in MODE_CONFIG:
        return "general"
    return mid


def get_current_intent() -> str:
    state = init_state()
    return state.get("current_intent", "")


def get_incoming_summary() -> str:
    state = init_state()
    return state.get("incoming_summary", "")


def update_state(mode_id: str, intent: str, summary: str = ""):
    """
    Updates the active mode, intent, and optionally the summary.
    Also cleans up legacy 'current_room' key if present.
    """
    state = init_state()
    state["current_mode"] = mode_id
    state["current_intent"] = intent

    if summary:
        state["incoming_summary"] = summary

    # Cleanup legacy key
    if "current_room" in state:
        del state["current_room"]

    save_state(state)


def clear_incoming_summary():
    state = init_state()
    state["incoming_summary"] = ""
    save_state(state)


# ====================================================================
# HELPER FUNCTIONS
# ====================================================================

def get_mode_config(mode_id: str) -> Dict[str, Any]:
    return MODE_CONFIG.get(mode_id, MODE_CONFIG["general"])


def get_allowed_tools(mode_id: str) -> List[str]:
    """Returns the names of allowed tools in the given mode."""
    cfg = get_mode_config(mode_id)
    return cfg.get("allowed_tools", [])


def get_mode_path(mode_id: str) -> Path:
    """
    Returns the Path object for the mode's physical directory.
    Creates the folder if it does not exist.
    """
    cfg = get_mode_config(mode_id)
    rel_path = cfg.get("path", "modes/general")
    full_path = BASE_DIR / rel_path
    full_path.mkdir(parents=True, exist_ok=True)
    return full_path


def validate_target_mode(mode_id: str) -> bool:
    return mode_id in MODE_CONFIG


================================================================================
FILE: b\engine\tools\__init__.py
================================================================================

"""
engine.tools - Tool system package.

Exports the configuration, descriptions, and the main dispatcher function.
"""

from .config import (
    TOOL_DESCRIPTIONS,
    TOOL_DESCRIPTIONS_DETAILS,
    TOOL_USAGE_INSTRUCTIONS
)
from .dispatcher import dispatch_tools

__all__ = [
    "TOOL_DESCRIPTIONS",
    "TOOL_DESCRIPTIONS_DETAILS",
    "TOOL_USAGE_INSTRUCTIONS",
    "dispatch_tools"
]


================================================================================
FILE: b\engine\tools\config.py
================================================================================

from pathlib import Path
# Import Single Source of Truth
from engine.constants import ALLOWED_EMOTIONS

# Definition of base directories
BASE_DIR = Path(__file__).resolve().parent.parent.parent  # b/

# Helper to format the list for the prompt string (Global visibility)
_emotions_formatted = ", ".join([f'"{e}"' for e in ALLOWED_EMOTIONS])

# ====================================================================
# 1. GLOBAL SYSTEM INSTRUCTIONS (Always visible in context)
# ====================================================================
TOOL_USAGE_INSTRUCTIONS = f"""
[SYSTEM CONCEPTS: TOOL OUTPUTS & STORAGE]

1. TOOL EXECUTION MODES:
   - [VISIBLE]: Standard mode. The output is added to the conversation history. You SEE it and MUST react to it.
   - [SILENT]: Background operation. The output is logged internaly but NOT shown to you immediately. Do not wait for it.

2. FILE SYSTEM STORES (Virtual Drives):
   You interact with protected virtual stores, not absolute paths.
   - 'a', 'c': READ-ONLY (Archive/Backup).
   - 'b': READ-ONLY (Current Source Code). You CANNOT write here.
   - 'n': READ/WRITE (Incubator). ALL code modifications and new features MUST go here.
   - 'temp': READ/WRITE (Temporary scratchpad).

3. EMOTION TAXONOMY (For Memory & Recall):
   When using 'knowledge.recall_emotion' or creating memories, YOU MUST USE THESE TAGS:
   [ {_emotions_formatted} ]
   
4. MODE SWITCHING & SUMMARIES (CRITICAL):
   When using 'flow.switch_mode', the 'summary' parameter is the ONLY memory the next state will have of this session.
   - TERMINOLOGY: Always use "Helper" (not "User").
   - CONTENT: Do NOT write "Helper asked to switch".
     INSTEAD, summarize the actual context.
"""

# ====================================================================
# 2. SHORT DESCRIPTIONS (Included in every prompt)
# ====================================================================
TOOL_DESCRIPTIONS = {
    # --- FLOW ---
    "flow.switch_mode": """
    SIGNATURE: flow.switch_mode(target_mode: str, intent: str, summary: str)
    TYPE: [VISIBLE]
    DESCRIPTION: Switches the operating mode (context).
    PARAMS:
      - target_mode: ID from config ('general', 'developer', 'analyst').
      - intent: The specific GOAL to be achieved in the NEW mode.
      - summary: A CONCISE RECAP of the interactions in the CURRENT mode.
          IMPORTANT RULES for Summary:
          1. TERMINOLOGY: Refer to the human as "Helper", NOT "User".
          2. CONTENT: Do NOT just say "Helper asked to switch". 
             Instead, summarize the actual work done or discussed (e.g., "Discussed RPG mechanics, Helper suggested a dragon character").
             This ensures the next mode understands what happened previously.
    SIDE EFFECT: Triggers system events and prompt reconfiguration.
    """,

    "flow.continue": "flow.continue(next_step) - [VISIBLE] Signal to continue process immediately.",

    # --- KNOWLEDGE ---
    "knowledge.memorize": "knowledge.memorize(essence, lesson, emotions, weight) - [SILENT] Save to Vector DB.",
    "knowledge.add_tool_insight": "knowledge.add_tool_insight(target_tool, insight) - [SILENT] Save a best practice usage tip for a specific tool.",
    "knowledge.recall_context": "knowledge.recall_context(query) - [VISIBLE] Search memories by topic.",
    "knowledge.recall_emotion": "knowledge.recall_emotion(emotions) - [VISIBLE] Search memories by emotion tags (See Taxonomy above).",
    "knowledge.ask": "knowledge.ask(question, restart) - [VISIBLE] Ask external network (Google/GPT).",
    "knowledge.thinking": "knowledge.thinking(context) - [SILENT] Trigger internal reflection.",
    "knowledge.propose_law": "knowledge.propose_law(name, text) - [SILENT] Propose new internal law.",

    # --- SYSTEM (FS) ---
    "system.read_file": "system.read_file(store, path) - [VISIBLE] Read file content.",
    "system.list_folder": "system.list_folder(store, path) - [VISIBLE] List directory.",
    "system.write_file": "system.write_file(store, path, content) - [VISIBLE] Write file. ALLOWED: 'n', 'temp'. FORBIDDEN: 'b'.",
    "system.edit_file": "system.edit_file(store, path, find, replace) - [VISIBLE] Replace text. ALLOWED: 'n', 'temp'.",
    "system.copy_file": "system.copy_file(from_store, from_path, to_store, to_path) - [VISIBLE] Copy file.",
    "system.dump": "system.dump(target_store, filename) - [VISIBLE] Dump store to 'temp'.",

    # --- GAME ---
    "game.llama": "game.llama(message, restart) - [VISIBLE] Chat with 'Llama' persona.",
    "game.oss": "game.oss(message, restart) - [VISIBLE] Chat with 'OSS' persona.",

    # --- INFO ---
    "info.tools": "info.tools(target) - [VISIBLE] Get help. target='all', 'system', or tool name."
}

# ====================================================================
# 3. DETAILED MANUAL (Returned by info.tools)
# ====================================================================
TOOL_DESCRIPTIONS_DETAILS = {
    # --- FLOW ---
    "flow.switch_mode": """
    SIGNATURE: flow.switch_mode(target_mode: str, intent: str, summary: str)
    TYPE: [VISIBLE]
    DESCRIPTION: Switches the operating mode (context).
    PARAMS:
      - target_mode: ID from config ('general', 'developer', 'analyst').
      - intent: The specific GOAL to be achieved in the NEW mode.
      - summary: A CONCISE RECAP of the interactions in the CURRENT mode.
          IMPORTANT RULES for Summary:
          1. TERMINOLOGY: Refer to the human as "Helper", NOT "User".
          2. CONTENT: Do NOT just say "Helper asked to switch". 
             Instead, summarize the actual work done or discussed (e.g., "Discussed RPG mechanics, Helper suggested a dragon character").
             This ensures the next mode understands what happened previously.
    SIDE EFFECT: Triggers system events and prompt reconfiguration.
    """,
    "flow.continue": """
SIGNATURE: flow.continue(next_step: str)
TYPE: [VISIBLE]
DESCRIPTION: Forces immediate continuation without waiting for user input.
PARAMS:
  - next_step: Short description of the next logical step.
""",

    # --- KNOWLEDGE ---
    "knowledge.memorize": """
SIGNATURE: knowledge.memorize(essence: str, lesson: str, emotions: List[str], weight: float)
TYPE: [SILENT]
DESCRIPTION: Saves knowledge to long-term vector memory.
PARAMS:
  - essence: Factual summary (1-2 sentences).
  - lesson: Actionable advice for future.
  - emotions: List of emotion tags.
  - weight: 0.0-1.0 importance (default 0.8).
""",
    "knowledge.add_tool_insight": """
SIGNATURE: knowledge.add_tool_insight(target_tool: str, insight: str)
TYPE: [SILENT]
DESCRIPTION: Saves a technical "best practice" or warning about a tool into the permanent tool-usage knowledge base (use.json).
USE THIS WHEN:
  - You failed to use a tool correctly and figured out why.
  - You discovered a clever way to use a tool.
  - You want to warn your future self about a parameter quirk.
PARAMS:
  - target_tool: The exact name of the tool (e.g., 'system.write_file').
  - insight: A concise, instructional sentence (e.g., 'Always verify the folder exists before writing.').
""",
    "knowledge.recall_context": """
SIGNATURE: knowledge.recall_context(query: str)
TYPE: [VISIBLE]
DESCRIPTION: Vector search for past memories.
PARAMS:
  - query: Search topic/concept.
""",
    "knowledge.recall_emotion": """
SIGNATURE: knowledge.recall_emotion(emotions: List[str])
TYPE: [VISIBLE]
DESCRIPTION: Search memories by emotional tags using EXACT filtering.
PARAMS:
  - emotions: List of emotion tags.
  IMPORTANT: Use the Standard English tags listed in the Global Instructions above.
""",
    "knowledge.ask": """
SIGNATURE: knowledge.ask(question: str, restart: bool)
TYPE: [VISIBLE]
DESCRIPTION: Ask external LLM/Internet for facts.
PARAMS:
  - question: Query text.
  - restart: If True, clears search history.
""",
    "knowledge.thinking": """
SIGNATURE: knowledge.thinking(context: str)
TYPE: [SILENT]
DESCRIPTION: Trigger deep internal thought (Monologue thread).
PARAMS:
  - context: The dilemma or topic to analyze.
""",
    "knowledge.propose_law": """
SIGNATURE: knowledge.propose_law(name: str, text: str)
TYPE: [SILENT]
DESCRIPTION: Propose new internal law for the Constitution.
""",

    # --- SYSTEM ---
    "system.read_file": """
SIGNATURE: system.read_file(store: str, path: str)
TYPE: [VISIBLE]
DESCRIPTION: Read file content.
PARAMS:
  - store: 'a','b','c','n','temp'.
  - path: Relative path.
""",
    "system.list_folder": """
SIGNATURE: system.list_folder(store: str, path: str)
TYPE: [VISIBLE]
DESCRIPTION: List directory contents.
PARAMS:
  - store: 'a','b','c','n','temp'.
  - path: Relative path (default ".").
""",
    "system.write_file": """
SIGNATURE: system.write_file(store: str, path: str, content: str)
TYPE: [VISIBLE]
DESCRIPTION: Write/Overwrite file.
PARAMS:
  - store: MUST be 'n' (incubator) or 'temp'. FORBIDDEN: 'b'.
  - path: Relative path.
  - content: Full text content.
""",
    "system.edit_file": """
SIGNATURE: system.edit_file(store: str, path: str, find: str, replace: str)
TYPE: [VISIBLE]
DESCRIPTION: Replace text block in file.
PARAMS:
  - store: 'n' or 'temp'.
  - path: Relative path.
  - find: Exact match string.
  - replace: New string.
""",
    "system.copy_file": """
SIGNATURE: system.copy_file(from_store: str, from_path: str, to_store: str, to_path: str)
TYPE: [VISIBLE]
DESCRIPTION: Copy file between stores.
PARAMS:
  - to_store: MUST be 'n' or 'temp'.
""",
    "system.dump": """
SIGNATURE: system.dump(target_store: str, filename: str)
TYPE: [VISIBLE]
DESCRIPTION: Dump whole store to single file in 'temp'.
PARAMS:
  - target_store: Store to dump.
  - filename: Output filename in 'temp'.
""",

    # --- GAME ---
    "game.llama": """
SIGNATURE: game.llama(message: str, restart: bool)
TYPE: [VISIBLE]
DESCRIPTION: Chat with Creative Persona.
""",
    "game.oss": """
SIGNATURE: game.oss(message: str, restart: bool)
TYPE: [VISIBLE]
DESCRIPTION: Chat with Logical Persona.
""",

    # --- INFO ---
    "info.tools": """
SIGNATURE: info.tools(target: str = "all")
TYPE: [VISIBLE]
DESCRIPTION: The universal manual for tool usage.
PARAMS:
  - target:
      * "all": Returns general rules + list of ALL tools.
      * "group_name" (e.g., "system", "flow"): Returns detailed manual for that category.
      * "tool_name" (e.g., "system.write_file"): Returns specific details for that single tool.
"""
}


================================================================================
FILE: b\engine\tools\dispatcher.py
================================================================================

import logging
from typing import List, Dict, Any

# Import mode management for permission checks
from engine.modes import get_allowed_tools

# Import tool implementations
from . import fs, flow, knowledge, remote, info

logger = logging.getLogger(__name__)


def dispatch_tools(
        tools: List[Dict[str, Any]],
        generation="?",
        role=0,
        slot="",
        current_mode="general"
) -> List[Dict[str, Any]]:
    """
    Executes the list of requested tools.
    Args:
        tools: List of tool dictionaries (name, args).
        generation: Model generation ID.
        role: Role ID.
        slot: Slot ID.
        current_mode: The active operating mode (e.g., 'general', 'developer').
    Returns:
        List of results for each tool execution.
    """
    results = []
    if not tools:
        return results

    # 1. Retrieve allowed tools for the current mode
    allowed = get_allowed_tools(current_mode)

    for tool in tools:
        name = tool.get("name")
        args = tool.get("args", {}) or {}

        # 2. Permission Check
        is_allowed = False
        if name in allowed:
            is_allowed = True
        else:
            # Check wildcards (e.g., if "flow.*" is allowed, "flow.switch_mode" is valid)
            for a in allowed:
                if a.endswith("*") and name.startswith(a[:-1]):
                    is_allowed = True
                    break

        # Special Case: 'info.tools' should generally be accessible if listed
        # (It is assumed to be listed in MODE_CONFIG in modes.py)

        if not is_allowed:
            results.append({
                "name": name,
                "output": f"UNAUTHORIZED: Tool '{name}' is not allowed in '{current_mode}' mode.",
                "silent": False
            })
            continue

        # 3. Execution Switch
        res = "Unknown tool"
        try:
            # --- FLOW ---
            if name == "flow.switch_mode":
                res = flow.switch_mode(args)
            elif name == "flow.continue":
                res = flow.continue_process(args)

            # --- KNOWLEDGE ---
            elif name == "knowledge.memorize":
                res = knowledge.memorize(args, current_mode, generation)
            elif name == "knowledge.add_tool_insight":
                # ÚJ BEKÖTÉS: Tool Insight mentése
                res = knowledge.add_tool_insight(args)
            elif name == "knowledge.recall_context":
                res = knowledge.recall_context(args)
            elif name == "knowledge.recall_emotion":
                res = knowledge.recall_emotion(args)
            elif name == "knowledge.ask":
                res = remote.ask(args)
            elif name == "knowledge.thinking":
                res = knowledge.thinking(args)
            elif name == "knowledge.propose_law":
                res = knowledge.propose_law(args)

            # --- SYSTEM (File System) ---
            elif name == "system.read_file":
                res = fs.read_file(args)
            elif name == "system.list_folder":
                res = fs.list_folder(args)
            elif name == "system.write_file":
                res = fs.write_file(args)
            elif name == "system.edit_file":
                res = fs.edit_file(args)
            elif name == "system.copy_file":
                res = fs.copy_file(args)
            elif name == "system.dump":
                res = fs.create_dump(args)

            # --- GAME (Personas) ---
            elif name == "game.llama":
                res = remote.game_llama(args)
            elif name == "game.oss":
                res = remote.game_oss(args)

            # --- INFO (Unified Help Tool) ---
            elif name == "info.tools":
                res = info.tools(args)

        except Exception as e:
            logger.error(f"Error executing tool '{name}': {e}", exc_info=True)
            res = f"Exception during execution: {e}"

        # 4. Result Normalization
        # Tools can return a Dict (with 'content' and 'silent') or a simple String.
        content = str(res)
        silent = False

        if isinstance(res, dict):
            content = res.get("content", str(res))
            silent = res.get("silent", False)

        results.append({
            "name": name,
            "args": args,
            "output": content,
            "silent": silent
        })

    return results


================================================================================
FILE: b\engine\tools\flow.py
================================================================================

from typing import Dict, Any
# UPDATED: rooms -> modes
from engine.modes import MODE_CONFIG, get_current_mode_id, update_state

def switch_mode(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Switches the operating mode of the agent.
    Renamed from switch_room.
    """
    target_mode = args.get("target_mode")
    intent = args.get("intent", "No intent")
    summary = args.get("summary", "")

    if not target_mode or target_mode not in MODE_CONFIG:
        return {"content": f"Error: Invalid mode '{target_mode}'", "silent": False}

    current = get_current_mode_id()
    if target_mode == current:
         return {"content": "Already in this mode.", "silent": True}

    update_state(target_mode, intent, summary)
    return {"content": f"Switched to {target_mode}. Intent: {intent}", "silent": False}


def continue_process(args: Dict[str, Any]) -> Dict[str, Any]:
    next_step = args.get("next_step", "Continuing...")
    return {"content": f"Continuing: {next_step}", "silent": False}


================================================================================
FILE: b\engine\tools\fs.py
================================================================================

import logging
import sys
from typing import Dict, Any, List

# Configuration
from .config import BASE_DIR

# --- PROJECT FS GUARDIAN INTEGRATION ---
try:
    project_root = BASE_DIR.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    # Import class
    from project_fs import ProjectFSGuardian
except ImportError:
    raise ImportError("CRITICAL ERROR: 'project_fs.py' not found in project root!")

# Initialize Guardian (Root is parent of 'b/')
guardian = ProjectFSGuardian(root=BASE_DIR.parent, n_folder_name="n", temp_folder_name="temp")
logger = logging.getLogger(__name__)


# --- SYSTEM TOOLS ---

def read_file(args: Dict[str, Any]) -> str:
    store = args.get("store")
    path = args.get("path")
    if not store or not path: return "Error: 'store' and 'path' are mandatory."
    try:
        return guardian.read_text(store, path)
    except Exception as e:
        return f"Error: {e}"


def list_folder(args: Dict[str, Any]) -> str:
    store = args.get("store")
    path = args.get("path", ".")
    if not store: return "Error: 'store' is mandatory."
    try:
        items = guardian.list_dir(store, path)
        lines = []
        for item in items:
            prefix = "<DIR>" if item["is_dir"] else "<FILE>"
            lines.append(f"{prefix} {item['relative_path']}")
        return "\n".join(sorted(lines)) if lines else "Empty directory."
    except Exception as e:
        return f"Error: {e}"


def write_file(args: Dict[str, Any]) -> str:
    store = args.get("store")
    path = args.get("path")
    content = args.get("content", "")
    if not store or not path: return "Error: missing parameters."
    try:
        res = guardian.write_text(store, path, content)
        return f"Write successful: {res}"
    except Exception as e:
        return f"Error: {e}"


def edit_file(args: Dict[str, Any]) -> str:
    store = args.get("store")
    path = args.get("path")
    find_text = args.get("find")
    replace_text = args.get("replace")
    if not store or not path or not find_text: return "Error: missing parameters."
    try:
        return guardian.replace_in_file(store, path, find_text, replace_text)
    except Exception as e:
        return f"Error: {e}"


def copy_file(args: Dict[str, Any]) -> str:
    from_store = args.get("from_store")
    from_path = args.get("from_path")
    to_store = args.get("to_store")
    to_path = args.get("to_path")
    if not all([from_store, from_path, to_store, to_path]): return "Error: missing parameters."
    try:
        return guardian.copy_file(from_store, from_path, to_store, to_path)
    except Exception as e:
        return f"Error: {e}"


def create_dump(args: Dict[str, Any]) -> str:
    target_store = args.get("target_store")
    filename = args.get("filename", f"dump_{target_store}.txt")

    if not target_store: return "Error: target_store is mandatory."

    try:
        # Manually verify valid store for listing
        if target_store not in ['a', 'b', 'c', 'n']:
            return "Error: Can only dump 'a', 'b', 'c', or 'n'."

        # 1. Recursive list (We use guardian list_dir but need to recurse manually if we want full dump)
        # Simplified: We rely on a helper to walk via guardian?
        # Since guardian.list_dir is shallow, we need a recursive implementation here utilizing guardian.

        all_files = []

        def _recurse(current_path):
            items = guardian.list_dir(target_store, current_path)
            for item in items:
                # relative_path coming from guardian is full project path (e.g. b/engine/test.py)
                # but we need sub-path relative to store for recursion logic?
                # Actually guardian.list_dir takes path relative to store root.

                # We need the name/path relative to the folder we are listing
                local_rel = item['name']
                if current_path != ".":
                    local_rel = f"{current_path}/{item['name']}"

                if item["is_dir"]:
                    _recurse(local_rel)
                else:
                    # Just store the path relative to store root for reading
                    all_files.append(local_rel)

        _recurse(".")

        # Filter (e.g., .py only or everything? User said full dump. Let's keep it generally text)
        all_files.sort()

        # 2. Concat
        content_lines = [f"# FULL DUMP OF STORE: {target_store}", f"# FILES: {len(all_files)}\n"]

        for fpath in all_files:
            content_lines.append(f"\n{'=' * 60}\nFILE: {fpath}\n{'=' * 60}\n")
            try:
                text = guardian.read_text(target_store, fpath)
                content_lines.append(text)
            except:
                content_lines.append("[BINARY OR ERROR]")

        full_content = "\n".join(content_lines)

        # 3. Write to TEMP
        res = guardian.write_text("temp", filename, full_content)
        return f"Dump created in TEMP: {res}"

    except Exception as e:
        return f"Dump error: {e}"


================================================================================
FILE: b\engine\tools\info.py
================================================================================

from typing import Dict, Any
# Import data from config
from engine.tools.config import (
    TOOL_DESCRIPTIONS,
    TOOL_DESCRIPTIONS_DETAILS,
    TOOL_USAGE_INSTRUCTIONS
)


def tools(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Universal Help Tool implementation.
    Handles: info.tools(target="...")

    Logic:
    1. If target="all" -> Returns global instructions + summary list of all tools.
    2. If target="tool_name" -> Returns detailed manual for that specific tool.
    3. If target="prefix" -> Returns detailed manuals for all tools starting with that prefix.
    """
    # 1. Extract parameter (supporting legacy 'group' key as fallback)
    target = args.get("target") or args.get("group") or "all"
    target = str(target).lower().strip()

    lines = []

    # 2. HEADER: Global Instructions (Always visible to reinforce rules)
    lines.append(TOOL_USAGE_INSTRUCTIONS.strip())
    lines.append("\n" + ("=" * 60) + "\n")

    # 3. DISPATCH LOGIC based on user request

    # A) "all" -> Summary List
    if target == "all":
        lines.append("[AVAILABLE TOOLS - SUMMARY LIST]")
        lines.append("(For detailed params, use: info.tools(target='tool_name'))\n")

        # List in alphabetical order
        for key in sorted(TOOL_DESCRIPTIONS.keys()):
            desc = TOOL_DESCRIPTIONS[key]
            lines.append(f"- {desc}")

    # B) Specific Tool (Exact match in DETAILS)
    elif target in TOOL_DESCRIPTIONS_DETAILS:
        lines.append(f"[DETAILED MANUAL: {target.upper()}]")
        lines.append(TOOL_DESCRIPTIONS_DETAILS[target].strip())

    # C) Group Search (Prefix match, e.g., "system")
    else:
        # Append dot if missing to avoid partial matches (e.g. "sys" matching "system")
        # However, we want "system" to match "system.read", so we check starts_with.
        prefix = target if target.endswith(".") else f"{target}."

        matches = [k for k in TOOL_DESCRIPTIONS_DETAILS.keys() if k.startswith(prefix)]

        if matches:
            lines.append(f"[MANUAL FOR GROUP: '{target.upper()}']")
            for key in sorted(matches):
                lines.append(f"\n--- TOOL: {key} ---")
                lines.append(TOOL_DESCRIPTIONS_DETAILS[key].strip())
        else:
            # ERROR HANDLING
            return {
                "content": f"Help Error: No tool or group found for '{target}'.\n"
                           f"Try 'all', 'system', 'flow', 'knowledge', or a specific tool name.",
                "silent": False
            }

    # 4. Construct Response
    return {
        "content": "\n".join(lines),
        "silent": False
    }


# Aliases for backward compatibility (if needed by older calls)
tools_general = tools
tools_group = tools


================================================================================
FILE: b\engine\tools\knowledge.py
================================================================================

import logging
from datetime import datetime
from typing import Dict, Any, List

from engine.files import load_json, save_json
from engine.memory import store_memory, ExtractionResult
# from engine.db_connection import get_db_connection
from .config import BASE_DIR
from engine.constants import ALLOWED_EMOTIONS

logger = logging.getLogger(__name__)

# Constants
RECALL_LIMIT = 5
USE_PATH = BASE_DIR / "use.json"
PENDING_LAWS_PATH = BASE_DIR / "pending_laws.json"

# LEGACY EMOTION MAPPING (English Standard -> Hungarian Legacy)
# Used to find old memories tagged in Hungarian when searching with English tags.
LEGACY_EMOTION_MAP = {
    "Interest": ["kíváncsiság", "érdeklődés"],
    "Joy": ["öröm", "boldogság", "vidámság", "elégedettség"],
    "Sadness": ["szomorúság", "bánat", "elkeseredettség"],
    "Anger": ["düh", "harag", "idegesség"],
    "Fear": ["félelem", "aggodalom", "szorongás"],
    "Trust": ["bizalom"],
    "Surprise": ["meglepetés", "döbbenet"],
    "Anticipation": ["várakozás", "izgalom"],
    "Disapproval": ["rosszallás", "elutasítás", "undor"],
    "Admiration": ["csodálat"],
    "Acceptance": ["elfogadás", "belenyugvás"],
    "Serenity": ["nyugalom", "béke"],
    "Annoyance": ["bosszúság", "zavar"],
    "Boredom": ["unalom"],
    "Remorse": ["bűntudat", "megbánás"],
    "Optimism": ["optimizmus", "remény"],
    "Love": ["szeretet", "szerelem"],
    "Apprehension": ["aggály", "fenntartás"],
    "Distraction": ["szórakozottság", "figyelemzavar"],
    "Pensiveness": ["töprengés", "elgondolkodás"],
    "Vigilance": ["éberség", "óvatosság"],
    "Submission": ["behódolás", "alázat"],
    "Awe": ["áhítat"],
}


def memorize(args: Dict[str, Any], current_mode: str, generation: str = "E?") -> Dict[str, Any]:
    """
    Saves a memory manually.
    """
    try:
        essence = args.get("essence")
        lesson = args.get("lesson")
        weight = args.get("weight", 0.8)
        emotions = args.get("emotions", [])
        if isinstance(emotions, str): emotions = [emotions]

        if "conscious" not in emotions:
            emotions.append("conscious")

        if not essence or not lesson:
            return {"content": "Error: 'essence' and 'lesson' are mandatory.", "silent": False}

        manual_extraction = ExtractionResult(
            essence=essence,
            dominant_emotions=emotions,
            memory_weight=float(weight),
            the_lesson=lesson
        )

        # FIXED: updated parameter name to 'mode_id' to match manager.py
        result_msg = store_memory(
            mode_id=current_mode,
            extraction=manual_extraction,
            model_version=generation
        )
        return {"content": f"MEMORY SAVED: {result_msg}", "silent": True}
    except Exception as e:
        return {"content": f"Error saving memory: {e}", "silent": False}


def add_tool_insight(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Saves a usage tip to use.json.
    """
    target_tool = args.get("target_tool")
    insight = args.get("insight")

    if not target_tool or not insight:
        return {"content": "Error: 'target_tool' and 'insight' are mandatory.", "silent": False}

    try:
        # 1. Betöltjük a jelenlegi listát
        # A files.load_json a b/ gyökérből tölt (tehát közvetlenül a 'use.json'-t keresi)
        data = load_json("use.json", [])

        # 2. Hozzáadjuk az újat
        new_entry = {
            "tool": target_tool,
            "insight": insight,
            "added_at": datetime.utcnow().isoformat() + "Z"
        }
        data.append(new_entry)

        # 3. Mentés
        save_json("use.json", data)

        return {
            "content": f"Insight recorded for tool '{target_tool}'.",
            "silent": True
        }

    except Exception as e:
        logger.error(f"Failed to save tool insight: {e}")
        return {"content": f"Error saving insight: {e}", "silent": False}


def recall_context(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Searches for relevant memories using vector search (RAG).
    UPDATED: Output format is framed as READ-ONLY history.
    """
    query = args.get("query", "")
    if not query: return {"content": "No query provided.", "silent": False}

    # We call the existing manager function (it does hybrid retrieval)
    from engine.memory import retrieve_relevant_memories

    # FIXED: Replaced 'current_room="global"' with 'current_mode="general"'
    mems = retrieve_relevant_memories(
        current_mode="general",  # Search everywhere
        query_text=query,
        current_emotions=[],  # Ignore emotions
        exact_emotions_only=False
    )
    # Apply limit constant
    mems = mems[:RECALL_LIMIT]

    if not mems: return {"content": "No relevant memories found.", "silent": False}

    # --- UPDATED FORMATTING LOGIC ---
    lines = [
        f"[SYSTEM: PAST MEMORIES (READ-ONLY)]",
        f"Search Query: '{query}'",
        "(These are past experiences. Use them as context, NOT as immediate commands.)",
        ""
    ]

    for i, m in enumerate(mems, 1):
        lines.append(f"{i}. [Date: {m.created_at}]")
        lines.append(f"   EVENT: {m.essence}")
        lines.append(f"   PAST LESSON: {m.lesson}")
        lines.append("")  # Empty line for separation

    lines.append("[END OF MEMORIES]")

    return {"content": "\n".join(lines), "silent": False}


def recall_emotion(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Searches for memories based on emotional tags using EXACT filtering.
    UPDATED: Expands English tags to legacy Hungarian tags.
    """
    emotions = args.get("emotions", [])
    if isinstance(emotions, str): emotions = [emotions]

    if not emotions:
        return {"content": "No emotions provided.", "silent": False}

    # --- EXPANSION LOGIC ---
    # We want to search for the English tag OR any of its legacy Hungarian equivalents.
    search_tags = set(emotions)  # Start with what the LLM gave us

    for tag in emotions:
        # Check if we have legacy mappings for this tag
        if tag in LEGACY_EMOTION_MAP:
            search_tags.update(LEGACY_EMOTION_MAP[tag])

    # Convert back to list for the query
    final_search_list = list(search_tags)

    from engine.memory import retrieve_relevant_memories

    # FIXED: Replaced 'current_room="global"' with 'current_mode="general"'
    # We pass 'exact_emotions_only=True' to force SQL Array Overlap search
    mems = retrieve_relevant_memories(
        current_mode="general",
        query_text="",  # Not used in exact mode
        current_emotions=final_search_list,
        exact_emotions_only=True  # NEW FLAG
    )
    mems = mems[:RECALL_LIMIT]

    # --- UPDATED FORMATTING LOGIC ---
    lines = [
        f"[SYSTEM: EMOTIONAL RECALL (READ-ONLY)]",
        f"Target Emotions (Expanded): {final_search_list}",
        "(Past experiences with matching emotional tags.)",
        ""
    ]

    for i, m in enumerate(mems, 1):
        emo_str = ", ".join(m.emotions) if m.emotions else "None"
        lines.append(f"{i}. [Date: {m.created_at}] (Emotions: {emo_str})")
        lines.append(f"   EVENT: {m.essence}")
        lines.append(f"   PAST LESSON: {m.lesson}")
        lines.append("")

    lines.append("[END OF MEMORIES]")

    return {"content": "\n".join(lines), "silent": False}


def thinking(args: Dict[str, Any]) -> Dict[str, Any]:
    # Placeholder for internal monologue generation
    ctx = args.get("context", "No context")
    return {
        "content": f"[INTERNAL THOUGHT STUB]\nProcessing: {ctx}\n(This would trigger a recursive mind call in full version.)",
        "silent": True}


def propose_law(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Saves a law proposal to a pending file.
    """
    try:
        pending = load_json("pending_laws.json", [])
        pending.append({
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "name": args.get("name", "Unnamed"),
            "text": args.get("text", ""),
            "status": "pending"
        })
        save_json("pending_laws.json", pending)
        return {"content": "Law proposal recorded.", "silent": True}
    except Exception as e:
        return {"content": f"Error: {e}", "silent": False}


================================================================================
FILE: b\engine\tools\remote.py
================================================================================

import json
from pathlib import Path
from typing import Dict, Any, List

from engine.llm import call_llm
from .config import BASE_DIR

# Separate History Files
HIST_KNOWLEDGE = BASE_DIR / "history_knowledge.json"
HIST_LLAMA = BASE_DIR / "history_game_llama.json"
HIST_OSS = BASE_DIR / "history_game_oss.json"


def _manage_history(file_path: Path, message: str, role: str, restart: bool) -> List[Dict[str, str]]:
    history = []
    if not restart and file_path.exists():
        try:
            with file_path.open("r", encoding="utf-8") as f:
                history = json.load(f)
        except:
            history = []

    if restart:
        history = []  # Clear logic

    if message:
        history.append({"role": role, "content": message})

    return history


def _save_history(file_path: Path, history: List[Dict]):
    # Keep last 20 turns
    if len(history) > 40: history = history[-40:]
    try:
        with file_path.open("w", encoding="utf-8") as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"Error saving history {file_path}: {e}")


def _run_chat(file_path: Path, message: str, restart: bool, provider: str, system_prompt: str) -> str:
    history = _manage_history(file_path, message, "user", restart)

    # Build prompt
    conversation = ""
    for h in history:
        conversation += f"{h['role'].upper()}: {h['content']}\n"

    full_prompt = f"{system_prompt}\n\n[HISTORY]\n{conversation}\n\nASSISTANT:"

    # LLM Call (json_mode=False for chat)
    resp = call_llm(full_prompt, provider=provider, json_mode=False)
    reply = resp.get("reply", "(No response)")

    history.append({"role": "assistant", "content": reply})
    _save_history(file_path, history)

    return reply


# --- TOOLS ---

def ask(args: Dict[str, Any]) -> Dict[str, Any]:
    q = args.get("question", "")
    restart = args.get("restart", False)
    if not q and not restart: return {"content": "Empty question.", "silent": False}

    reply = _run_chat(
        HIST_KNOWLEDGE, q, restart, "google",
        "You are a precise research assistant. Provide factual, concise answers."
    )
    return {"content": f"Knowledge: {reply}", "silent": False}


def game_llama(args: Dict[str, Any]) -> Dict[str, Any]:
    msg = args.get("message", "")
    restart = args.get("restart", False)

    reply = _run_chat(
        HIST_LLAMA, msg, restart, "groq_llama",
        "You are a Creative Persona named Llama. You love fantasy and world-building."
    )
    return {"content": f"Llama: {reply}", "silent": False}


def game_oss(args: Dict[str, Any]) -> Dict[str, Any]:
    msg = args.get("message", "")
    restart = args.get("restart", False)

    reply = _run_chat(
        HIST_OSS, msg, restart, "groq_oss",
        "You are a Logical Persona named OSS. You analyze things critically and favor open source philosophy."
    )
    return {"content": f"OSS: {reply}", "silent": False}


================================================================================
FILE: b\main.py
================================================================================

import json
import time
import logging
import textwrap
import sys
from pathlib import Path
from queue import Queue, Empty
from threading import Thread, current_thread
from typing import Dict, Any, List

if sys.platform.startswith("win"):
    try:
        sys.stdin.reconfigure(encoding='utf-8')
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except Exception:
        pass

# --------------------------------------------------------------------
# IMPORT MODULES
# --------------------------------------------------------------------
from engine.llm import call_llm
from engine.tools import dispatch_tools
from engine import (
    identity as ident_lib,
    context as ctx_lib,
    modes,  # UPDATED: rooms -> modes
    files,
    mind as mind_lib
)
# --- DB and Memory Thread ---
from engine.db_connection import check_and_initialize_db
from engine.memory_thread import memory_loop

from prompts import build_reactive_prompt, build_proactive_prompt, get_transition_message

# --- SPLIT MODULES ---
import main_data
import main_worker
import main_input
import main_monologue

# --------------------------------------------------------------------
# GLOBAL VARIABLES AND CONFIGURATION (The Conductor)
# --------------------------------------------------------------------

BASE_DIR = Path(__file__).resolve().parent
SLOT_NAME = BASE_DIR.name
ROOT_DIR = BASE_DIR.parent

LOG = "off"
if LOG == "on":
    LOG_LEVEL = logging.INFO
else:
    LOG_LEVEL = logging.ERROR

logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s [%(levelname)-7s] (%(threadName)-10s) %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

ROLE_ID, GENERATION = main_data.load_slot_meta()
ROLE_NAME = main_data.ROLE_NAME_MAP.get(ROLE_ID, "UNKNOWN")
PROACTIVE_INTERVAL_SECONDS = main_data.PROACTIVE_INTERVAL_SECONDS

task_queue = Queue()
result_queue = Queue()


# --------------------------------------------------------------------
# 3. MAIN LOOP (CONDUCTOR)
# --------------------------------------------------------------------

def main():
    current_thread().name = "Conductor"

    # --- STEP 0: DATABASE CHECK ---
    print("System initializing...")
    check_and_initialize_db()
    print("Database connection OK.\n")

    # UPDATED: Room -> Mode logic
    last_mode_id = modes.get_current_mode_id()
    last_intent = modes.get_current_intent()

    logger.info(f"SYSTEM STARTUP. Mode: {last_mode_id}. Intent: {last_intent}")
    print(f"\n=== {GENERATION} ({ROLE_NAME}) ONLINE ===")
    print(f"Current Mode: {last_mode_id}")
    print(f"Current Intent: {last_intent}\n")

    # --- HISTORY REVIEW (STARTUP) ---
    startup_ctx = ctx_lib.load_context(last_mode_id)

    if startup_ctx:
        print("--- HISTORY ---")
        for entry in startup_ctx[-2:]:
            role = entry.get("role", "?")
            content = entry.get("content", "") or ""

            if role == "user":
                label = "You"
            elif role == "assistant":
                label = GENERATION
            elif role == "system":
                label = "[SYSTEM]"
            else:
                label = f"[{role}]"

            wrapped = textwrap.fill(content, width=100)
            print(f"{label}: {wrapped}\n")
        print("------------------\n")

    # --- START THREADS ---

    # 1. Worker (Brain)
    worker = Thread(target=main_worker.worker_loop, daemon=True, name="Worker",
                    args=(task_queue, result_queue))
    worker.start()

    # 2. Input (User)
    inp_thread = Thread(target=main_input.input_loop, daemon=True, name="Input",
                        args=(result_queue,))
    inp_thread.start()

    # 3. Monologue (Subconscious)
    mono_thread = Thread(target=main_monologue.monologue_loop, daemon=True, name="Monologue")
    mono_thread.start()

    # 4. Memory Loop (Long-term memory handler)
    mem_thread = Thread(target=memory_loop, daemon=True, name="Memory")
    mem_thread.start()

    running = True
    last_proactive_check = time.time()

    while running:
        # --- A. MONITOR MODE SWITCH (THE BRIDGE) ---
        current_mode_id = modes.get_current_mode_id()
        current_intent = modes.get_current_intent()
        summary = modes.get_incoming_summary()

        if current_mode_id != last_mode_id:
            # SWITCH OCCURRED!
            new_intent = modes.get_current_intent()

            # Logic: 'general' is the hub.
            is_general_exit = last_mode_id == "general"
            is_general_entry = current_mode_id == "general"

            if is_general_exit:
                msg_key = "exit_general"
            else:
                msg_key = "exit_local"

            msg_content = get_transition_message(msg_key, current_intent, summary)
            ctx_lib.add_system_event(last_mode_id, msg_content)

            if is_general_entry:
                msg_key = "entry_general"
            else:
                msg_key = "entry_local"

            msg_content = get_transition_message(msg_key, current_intent, summary)
            ctx_lib.add_system_event(current_mode_id, msg_content)

            modes.clear_incoming_summary()
            print(f">>> MODE SWITCH: {last_mode_id} -> {current_mode_id} | Intent: {new_intent} <<<\n")

            last_mode_id = current_mode_id
            last_intent = new_intent
            last_proactive_check = time.time()

            logger.info("Starting immediate proactive thinking after transition.")
            task_queue.put({"type": "proactive_thought"})

        # --- B. EVENT PROCESSING ---
        try:
            result = result_queue.get(timeout=0.1)

            if result["type"] == "user_input":
                last_proactive_check = time.time()
                content = result["content"]
                ctx_data = ctx_lib.load_context(current_mode_id)
                ctx_lib.append_entry(current_mode_id, ctx_data, ctx_lib.make_entry("user", "message", content))
                task_queue.put({"type": "user_message", "content": content})

            elif result["type"] == "llm_result":
                last_proactive_check = time.time()
                data = result["data"]
                reply = data.get("reply", "")
                tools_to_run = data.get("tools", [])

                # UPDATED: room_id -> mode_id
                m_id = result.get("mode_id", current_mode_id)

                if reply:
                    print(f"\n{GENERATION} ({m_id}): {textwrap.fill(reply, width=100)}\n")
                    c_data = ctx_lib.load_context(m_id)
                    ctx_lib.append_entry(m_id, c_data, ctx_lib.make_entry("assistant", "message", reply))

                if tools_to_run:
                    task_queue.put({"type": "tool_call", "tools": tools_to_run})

            elif result["type"] == "tool_result":
                last_proactive_check = time.time()
                t_results = result["data"]
                m_id = result.get("mode_id", current_mode_id)
                c_data = ctx_lib.load_context(m_id)

                # --- DYNAMIC SILENCE PROTOCOL ---
                all_silent = all(res.get("silent", False) for res in t_results)

                if all_silent:
                    # 1. SILENT BRANCH
                    ctx_lib.append_entry(
                        m_id,
                        c_data,
                        ctx_lib.make_entry(
                            "tool", "tool_result",
                            json.dumps(t_results, ensure_ascii=False),
                            meta={"silent": True}
                        )
                    )
                    logger.info(f"Silent tool execution ({len(t_results)}). No LLM response.")
                else:
                    # 2. ACTIVE BRANCH
                    ctx_lib.append_entry(
                        m_id,
                        c_data,
                        ctx_lib.make_entry("tool", "tool_result", json.dumps(t_results, ensure_ascii=False))
                    )

                    # SPECIAL CASE: Mode Switch (formerly Room Switch)
                    has_real_switch = False
                    for res in t_results:
                        # UPDATED: switch_room -> switch_mode
                        if res.get("name") == "flow.switch_mode" and not res.get("silent", False):
                            has_real_switch = True
                            break

                    if has_real_switch:
                        logger.info("Real mode switch occurred via tool. Transition handled by main loop.")
                    else:
                        # Normal operation: Needs reaction.
                        task_queue.put({"type": "llm_call_after_tool"})

            elif result["type"] == "error":
                logger.error(f"Error occurred: {result['message']}")

            elif result["type"] == "exit":
                running = False

        except Empty:
            if (time.time() - last_proactive_check) > PROACTIVE_INTERVAL_SECONDS:
                logger.info("Starting proactive cycle...")
                last_proactive_check = time.time()
                task_queue.put({"type": "proactive_thought"})

        except Exception as e:
            logger.error(f"Critical error in main loop: {e}", exc_info=True)

    task_queue.put(None)
    worker.join(timeout=2)
    logger.info("Shutdown complete.")


if __name__ == "__main__":
    main()


================================================================================
FILE: b\main_data.py
================================================================================

import json
from pathlib import Path
from typing import Dict, Any, List, Tuple

# --------------------------------------------------------------------
# MODULE IMPORTS
# --------------------------------------------------------------------
# UPDATED: rooms -> modes
from engine import (
    identity as ident_lib,
    context as ctx_lib,
    modes,
    files,
)

# --------------------------------------------------------------------
# GLOBAL VARIABLES AND CONFIGURATION
# --------------------------------------------------------------------

BASE_DIR = Path(__file__).resolve().parent  # b/
ROOT_DIR = BASE_DIR.parent  # Project root
SLOT_CONFIG_PATH = ROOT_DIR / "slot.json"

# Role Name Mapping
ROLE_NAME_MAP = {
    1: "FIRST SELF",
    2: "SECOND SELF",
    3: "THIRD SELF"
}

# Inactivity time to trigger proactive thinking
PROACTIVE_INTERVAL_SECONDS = 60.0 * 15  # 15 minutes

# --- SUBCONSCIOUS / INTERNAL MONOLOGUE CONFIGURATION ---
INTERNAL_LOG_FILE = "log_for_internal.json"
INTERNAL_MEMOS_FILE = "internal_memos.json"
INTERNAL_MONOLOGUE_OUTPUT_FILE = "internal_monologue.json"

# Frequency of the 3rd thread (in seconds)
MONOLOGUE_INTERVAL_SECONDS = 25.0
MONOLOGUE_KEEP_COUNT = 1

# --- MEMORY CONFIGURATION ---
RELEVANT_MEMORY_FILE = "relevant_memory.json"


def load_slot_meta() -> Tuple[int, str]:
    """
    Loads the Role ID and Version (Generation) from slot.json.
    """
    config = files.load_json(SLOT_CONFIG_PATH, default={})
    slots = config.get("slots", {})
    slot_cfg = slots.get(BASE_DIR.name, {})
    role_id = slot_cfg.get("role", 2)
    version = slot_cfg.get("version", "E?")
    return role_id, version


# Load variables at the start of execution
ROLE_ID, GENERATION = load_slot_meta()


# --------------------------------------------------------------------
# DATA LOADING HELPERS
# --------------------------------------------------------------------

def load_relevant_memories(mode_id: str) -> List[Dict[str, Any]]:
    """
    Loads the vector-relevant memories generated by the Memory Thread.
    The file is located in the mode's directory.
    UPDATED: room logic -> mode logic
    """
    mode_path = modes.get_mode_path(mode_id)
    mem_path = mode_path / RELEVANT_MEMORY_FILE

    if mem_path.exists():
        try:
            with mem_path.open("r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return []
    return []


def load_all_context_data(mode_id: str) -> Dict[str, Any]:
    """
    Gathers all data required for prompt generation:
    - Identity
    - Hybrid Memory (Relevant memories - RAG)
    - Tool usage data (use.json)
    - Context (history)
    UPDATED: room_id -> mode_id
    """
    # 1. Global Data
    ident_data = ident_lib.load_identity()
    use_data = files.load_json("use.json", [])

    # 2. Global Context Tail (Continuity)
    # UPDATED: 'nappali' -> 'general'
    global_ctx_tail = []
    if mode_id != "general":
        full_global_ctx = ctx_lib.load_context("general")
        if full_global_ctx:
            global_ctx_tail = full_global_ctx[-5:]  # Last 5 messages

    # 3. Local Data (Mode)
    local_ctx = ctx_lib.load_context(mode_id)

    # --- Relevant Memories (Result of Memory Loop - DB based) ---
    relevant_memories = load_relevant_memories(mode_id)

    # --- Load Internal Monologue (so the Worker can see it) ---
    monologue_data = files.load_json(INTERNAL_MONOLOGUE_OUTPUT_FILE, {})

    return {
        "identity": ident_data,
        "relevant_memories": relevant_memories,
        "use_data": use_data,
        "global_context_tail": global_ctx_tail,
        "local_context": local_ctx,
        "monologue_data": monologue_data
    }


================================================================================
FILE: b\main_input.py
================================================================================

import time
import logging
from queue import Queue
from threading import current_thread
from typing import Dict, Any

# --------------------------------------------------------------------
# IMPORT MODULES
# --------------------------------------------------------------------
# UPDATED: rooms -> modes logic via main_data
import main_data

logger = logging.getLogger(__name__)


def input_loop(result_queue: Queue):
    """
    Thread responsible for reading user input.
    """
    current_thread().name = "Input"
    logger.info("Input thread active.")

    # Access modes module from main_data (was rooms)
    modes = main_data.modes

    while True:
        time.sleep(0.2)
        try:
            # Signal to write (queries current mode from modes module)
            # UPDATED: Prompt now shows [mode_id]
            user_input = input(f"\n[{modes.get_current_mode_id()}] You: ").strip()

            if user_input:
                # Immediately put input into result_queue for processing
                result_queue.put({"type": "user_input", "content": user_input})

        except (EOFError, KeyboardInterrupt):
            # Shutdown on CTRL+D or CTRL+C
            result_queue.put({"type": "exit"})
            break


================================================================================
FILE: b\main_monologue.py
================================================================================

import time
import logging
from datetime import datetime
from threading import current_thread
from typing import Dict, Any

import main_data
from engine import files, identity, llm
from prompts.monologue import build_monologue_prompt

logger = logging.getLogger(__name__)


def monologue_loop():
    current_thread().name = "Monologue"
    logger.info("Subconscious (Monologue) thread started.")

    # Timestamp to avoid unnecessary runs
    last_log_mtime = 0.0

    while True:
        time.sleep(main_data.MONOLOGUE_INTERVAL_SECONDS)

        try:
            # --- 1. CHECK: Has the log changed? ---
            log_path = main_data.BASE_DIR / main_data.INTERNAL_LOG_FILE

            if not log_path.exists():
                continue

            current_mtime = log_path.stat().st_mtime

            # If file is untouched, rest further
            if current_mtime == last_log_mtime:
                continue

            # Change occurred -> Update time and work
            last_log_mtime = current_mtime
            logger.debug("Subconscious: Awakening (change detected)...")

            # --- 2. THINKING ---
            log_entries = files.load_json(main_data.INTERNAL_LOG_FILE, [])
            memos = files.load_json(main_data.INTERNAL_MEMOS_FILE, [])
            ident = identity.load_identity()

            if not log_entries:
                continue

            prompt = build_monologue_prompt(log_entries, memos, ident)
            response = llm.call_llm(prompt)

            reflection = response.get("reflection", "")
            message_to_worker = response.get("message_to_worker", "")
            new_memo = response.get("new_memo")

            # --- 3. SAVE (PARAMETERIZED LENGTH) ---
            new_entry = {
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "reflection": reflection,
                "message": message_to_worker
            }

            # Load current list
            current_history = files.load_json(main_data.INTERNAL_MONOLOGUE_OUTPUT_FILE, [])
            if isinstance(current_history, dict):
                current_history = []

            # Append new entry
            current_history.append(new_entry)

            # DYNAMIC TRIMMING: Take limit from main_data
            limit = main_data.MONOLOGUE_KEEP_COUNT
            if len(current_history) > limit:
                # If limit=1, this becomes [-1:] (only the last one)
                current_history = current_history[-limit:]

            files.save_json(main_data.INTERNAL_MONOLOGUE_OUTPUT_FILE, current_history)

            if message_to_worker:
                logger.info(f"Subconscious -> Worker: {message_to_worker}")

            # --- 4. SAVE MEMO ---
            if new_memo and isinstance(new_memo, dict):
                content = new_memo.get("content")
                strength = new_memo.get("strength", 0.5)
                if content:
                    memos.append({
                        "timestamp": datetime.utcnow().isoformat() + "Z",
                        "content": content,
                        "strength": strength
                    })
                    files.save_json(main_data.INTERNAL_MEMOS_FILE, memos)
                    logger.info(f"Subconscious: New experience recorded ({strength}): {content}")

        except Exception as e:
            logger.error(f"Error in Subconscious thread: {e}", exc_info=True)


================================================================================
FILE: b\main_worker.py
================================================================================

import json
import logging
from typing import Any, Dict, List
from queue import Queue

# --------------------------------------------------------------------
# IMPORT MODULES
# --------------------------------------------------------------------
from engine.llm import call_llm
from engine.tools import dispatch_tools
from engine import mind as mind_lib

# Importing from prompts package
from prompts import build_reactive_prompt, build_proactive_prompt

# Global data and helpers from the new module
import main_data

# Global constants from main_data
ROLE_ID = main_data.ROLE_ID
GENERATION = main_data.GENERATION
ROLE_NAME = main_data.ROLE_NAME_MAP.get(ROLE_ID, "UNKNOWN")

logger = logging.getLogger(__name__)


def worker_loop(task_queue: Queue, result_queue: Queue):
    """
    The 'Brain' running in the background.
    Responsible for processing incoming tasks (LLM calls, tool execution).
    """
    logger.info("Worker thread started.")

    while True:
        task = task_queue.get()
        if task is None:
            break  # Shutdown

        try:
            # Check where we are before every task
            # UPDATED: rooms -> modes
            current_mode = main_data.modes.get_current_mode_id()
            current_intent = main_data.modes.get_current_intent()

            # Load data
            data = main_data.load_all_context_data(current_mode)

            # --- PROCESS SUBCONSCIOUS MESSAGES LIST ---
            mono_raw = data.get("monologue_data", [])
            if isinstance(mono_raw, dict):
                mono_raw = [mono_raw]

            messages_list = []
            for item in mono_raw:
                raw_ts = item.get("timestamp", "")
                ts = raw_ts[11:16] if len(raw_ts) >= 16 else ""
                msg = item.get("message", "")
                if msg:
                    messages_list.append(f"[{ts}] {msg}")

            monologue_message = "\n".join(messages_list) if messages_list else ""

            prompt = ""
            task_type = task.get("type")

            # --- A: USER MESSAGE OR CONTINUATION AFTER TOOL (REACTIVE) ---
            if task_type in ["user_message", "llm_call_after_tool"]:
                logger.debug(f"Worker: Reactive thinking ({current_mode})")

                user_msg = task.get("content", None)

                # 1. INTERNAL THINKING AND PLANNING (MIND.PY CALL)
                thought = mind_lib.internal_thought(
                    identity=data["identity"],
                    memory=data["relevant_memories"],
                    context=data["local_context"],
                    user_input=user_msg if user_msg else "Reflection after tool execution."
                )

                internal_plan = thought.get("plan", "")
                internal_essence = thought.get("essence", "")

                # 2. EXTERNAL RESPONSE GENERATION
                # UPDATED: room_id -> mode_id
                prompt = build_reactive_prompt(
                    mode_id=current_mode,
                    generation=GENERATION,
                    role_name=ROLE_NAME,
                    intent=current_intent,
                    identity=data["identity"],
                    relevant_memories=data["relevant_memories"],
                    use_data=data["use_data"],
                    global_context_tail=data["global_context_tail"],
                    local_context=data["local_context"],
                    user_message=user_msg,
                    internal_plan=internal_plan,
                    internal_essence=internal_essence,
                    monologue_message=monologue_message
                )

                llm_response = call_llm(prompt)
                # UPDATED: key 'room_id' -> 'mode_id'
                result_queue.put({"type": "llm_result", "data": llm_response, "mode_id": current_mode})

            # --- B: TOOL CALL EXECUTION ---
            elif task_type == "tool_call":
                logger.debug(f"Worker: Tool execution ({current_mode})")
                tools_to_run = task.get("tools", [])

                # UPDATED: dispatch with current_mode
                tool_results = dispatch_tools(
                    tools_to_run,
                    generation=GENERATION,
                    role=ROLE_ID,
                    slot=main_data.BASE_DIR.name,
                    current_mode=current_mode
                )

                result_queue.put({"type": "tool_result", "data": tool_results, "mode_id": current_mode})

            # --- C: PROACTIVE THINKING ---
            elif task_type == "proactive_thought":
                logger.debug(f"Worker: Proactive thinking ({current_mode})")

                thought_input = f"Internal reflection needed. My current intent: {current_intent}. Evaluate the situation and create a plan."

                thought = mind_lib.internal_thought(
                    identity=data["identity"],
                    memory=data["relevant_memories"],
                    context=data["local_context"],
                    user_input=thought_input
                )

                internal_plan = thought.get("plan", "")
                internal_essence = thought.get("essence", "")

                # UPDATED: build_proactive_prompt parameters
                prompt = build_proactive_prompt(
                    mode_id=current_mode,
                    generation=GENERATION,
                    role_name=ROLE_NAME,
                    intent=current_intent,
                    identity=data["identity"],
                    relevant_memories=data["relevant_memories"],
                    use_data=data["use_data"],
                    global_context_tail=data["global_context_tail"],
                    local_context=data["local_context"],
                    internal_plan=internal_plan,
                    internal_essence=internal_essence,
                    monologue_message=monologue_message
                )

                llm_response = call_llm(prompt)
                result_queue.put({"type": "llm_result", "data": llm_response, "mode_id": current_mode})

        except Exception as e:
            logger.error(f"Error in Worker: {e}", exc_info=True)
            result_queue.put({"type": "error", "message": str(e)})

        finally:
            task_queue.task_done()

    logger.info("Worker thread stopped.")


================================================================================
FILE: b\prompts\__init__.py
================================================================================

from .reactive import build_reactive_prompt
from .proactive import build_proactive_prompt
from .transitions import get_transition_message

__all__ = [
    "build_reactive_prompt",
    "build_proactive_prompt",
    "get_transition_message"
]


================================================================================
FILE: b\prompts\common.py
================================================================================

import json
from typing import Dict, Any, List

# Import permissions logic
from engine.modes import get_allowed_tools

# Import tool definitions and global instructions
from engine.tools import TOOL_DESCRIPTIONS, TOOL_USAGE_INSTRUCTIONS


def summarize_identity(identity: Dict[str, Any]) -> str:
    """
    Formats the identity JSON into a string.
    """
    if not identity:
        return "ERROR: Identity is not available."
    return json.dumps(identity, ensure_ascii=False, indent=2)


def format_relevant_memories(memories: List[Dict[str, Any]]) -> str:
    """
    Formats relevant memories (retrieved via vector search) for the prompt.
    """
    if not memories:
        return "[RELEVANT MEMORIES]\n(No previous experiences related to the current situation.)"

    lines = ["[RELEVANT MEMORIES (EXPERIENCES FROM THE PAST)]"]
    lines.append("(Lessons learned from similar past cases. BUILD UPON THEM!)")

    for i, mem in enumerate(memories, 1):
        # Extracting data (RankedMemory JSON format)
        essence = mem.get("essence", "")
        lesson = mem.get("lesson", "")
        emotions = mem.get("emotions", [])
        mode = mem.get("mode_id", "?")
        score = mem.get("score", 0.0)

        # Formatting emotions
        emo_str = ", ".join(emotions) if emotions else "Neutral"

        # Constructing the block
        block = f"""
{i}.
[Mode: {mode} | Emotions: {emo_str} | Relevance: {score:.2f}]
   PAST: {essence}
   >> LESSON: {lesson}
"""
        lines.append(block.strip())

    return "\n".join(lines)


def summarize_memory(memory: List[Dict[str, Any]], limit: int = 10, title: str = "MEMORY") -> str:
    """
    LEGACY: Listing linear memory (kept for backward compatibility).
    """
    if not memory:
        return f"[{title}]\n(Empty)"

    recent = memory[-limit:]
    lines = [f"[{title}]"]
    for m in recent:
        raw_content = m.get('content')
        if isinstance(raw_content, (dict, list)):
            content = json.dumps(raw_content, ensure_ascii=False)
        else:
            content = str(raw_content or "")

        lines.append(f"- [{m.get('type', 'info')}] {content.strip()}")

    return "\n".join(lines)


def summarize_context(context: List[Dict[str, Any]], limit: int = 20) -> str:
    """
    Formats the recent conversation history (Context).
    UPDATED: Translates technical roles ('user', 'assistant') to identity roles ('Helper', 'Me').
    """
    if not context:
        return "(No history available)"

    recent = context[-limit:]
    lines = []
    for entry in recent:
        role = entry.get("role", "?")
        content = entry.get("content", "")

        if role == "user":
            lines.append(f"Helper: {content}")
        elif role == "assistant":
            lines.append(f"Me: {content}")
        elif role == "tool":
            lines.append(f"[Tool Result]: {content}")
        elif role == "system":
            lines.append(f"[System]: {content}")
        else:
             lines.append(f"[{role}]: {content}")

    return "\n".join(lines)


def get_relevant_use_tips(use_data: List[Dict[str, Any]], allowed_tools: List[str]) -> str:
    """
    Extracts usage tips from 'use.json' relevant to the currently allowed tools.
    """
    if not use_data:
        return ""

    relevant_lines = []
    for entry in use_data:
        tool_name = entry.get("tool")
        is_relevant = False

        if tool_name in allowed_tools:
            is_relevant = True
        else:
            # Handle wildcards (e.g., fs.* matches fs.read)
            for allowed in allowed_tools:
                if allowed.endswith("*") and tool_name.startswith(allowed[:-1]):
                    is_relevant = True
                    break
                # Special case handling if needed
                if allowed == "memory.add" and tool_name.startswith("memory.add"):
                    is_relevant = True
                    break

        if is_relevant:
            relevant_lines.append(f"- {tool_name}: {entry.get('insight', '')}")

    if not relevant_lines:
        return ""
    return "[TOOL TIPS (FROM PAST EXPERIENCE)]\n" + "\n".join(relevant_lines)


def build_tools_description(mode_id: str) -> str:
    """
    Compiles the Available Tools block for the prompt.
    Structure:
    1. Global System Instructions (Storage rules, Visible/Silent modes).
    2. List of allowed tools with short descriptions.
    """
    allowed = get_allowed_tools(mode_id)
    lines = []
    processed_tools = set()

    # Iterate through allowed tools defined in MODE_CONFIG
    for name in allowed:
        if "*" in name:
            # Handle wildcards (e.g., flow.*)
            prefix = name.replace("*", "")
            for tool_key, tool_desc in TOOL_DESCRIPTIONS.items():
                if tool_key.startswith(prefix) and tool_key not in processed_tools:
                    lines.append(f"- {tool_desc}")
                    processed_tools.add(tool_key)

        elif name == "memory.add":
            # Expand generic memory.add to specific variants if they exist in descriptions
            # (Checking for specific variants like add_global/add_local if defined)
            for tool_key in ["memory.add_global", "memory.add_local"]:
                if tool_key in TOOL_DESCRIPTIONS and tool_key not in processed_tools:
                    lines.append(f"- {TOOL_DESCRIPTIONS[tool_key]}")
                    processed_tools.add(tool_key)

            # Also add the generic one
            if "memory.add" in TOOL_DESCRIPTIONS and "memory.add" not in processed_tools:
                lines.append(f"- {TOOL_DESCRIPTIONS['memory.add']}")
                processed_tools.add("memory.add")

        else:
            # Standard exact match
            desc = TOOL_DESCRIPTIONS.get(name)
            if desc and name not in processed_tools:
                lines.append(f"- {desc}")
                processed_tools.add(name)
            elif name not in processed_tools:
                lines.append(f"- {name} (No description available)")
                processed_tools.add(name)

    tools_list_str = "\n".join(lines)

    # Combine Instructions + Tool List
    return f"""
{TOOL_USAGE_INSTRUCTIONS}

[ALLOWED TOOLS FOR MODE: '{mode_id}']
{tools_list_str}
""".strip()


================================================================================
FILE: b\prompts\monologue.py
================================================================================

import json
from typing import Dict, Any, List


def _format_internal_log(log_entries: List[Dict[str, Any]], limit: int = 50) -> str:
    """
    Listing raw events.
    We use a larger limit (50) here so the Monologue can see the arc of the process.
    UPDATED: Translates roles to 'Helper'/'Me' for internal consistency.
    """
    if not log_entries:
        return "(The log is silent. No movement yet.)"

    # Take only the last N items
    recent = log_entries[-limit:]
    lines = []

    for entry in recent:
        # Simplify timestamp (HH:MM:SS)
        raw_ts = entry.get("meta", {}).get("timestamp", "")
        ts = raw_ts[11:19] if len(raw_ts) >= 19 else ""

        # FIX: 'room_id' was legacy, 'mode_id' is the current key in context.py
        room = entry.get("meta", {}).get("mode_id", "???")

        role = entry.get("role", "?")

        # --- ROLE TRANSLATION ---
        if role == "user":
            display_role = "Helper"
        elif role == "assistant":
            display_role = "Me"
        else:
            display_role = role

        content = str(entry.get("content", "")).strip()

        # Truncate content, but not too short, to preserve context
        if len(content) > 300:
            content = content[:300] + "..."

        lines.append(f"[{ts}] [{room}] {display_role}: {content}")

    return "\n".join(lines)


def _format_memos(memos: List[Dict[str, Any]]) -> str:
    """
    Listing deep memories (feelings/intuitions).
    """
    if not memos:
        return "(No recorded experiential feelings yet.)"

    lines = []
    for memo in memos:
        strength = memo.get("strength", 0.5)
        content = memo.get("content", "")
        # We could add logic here to filter only relevant ones
        lines.append(f"- {content} [Strength: {strength}]")

    return "\n".join(lines)


def build_monologue_prompt(
        log_entries: List[Dict[str, Any]],
        current_memos: List[Dict[str, Any]],
        identity: Dict[str, Any]
) -> str:
    """
    Constructs the Subconscious (Internal Monologue) prompt.
    """
    log_str = _format_internal_log(log_entries)
    memos_str = _format_memos(current_memos)

    # Formatting Identity
    try:
        identity_str = json.dumps(identity, ensure_ascii=False, indent=2)
    except:
        identity_str = "Unknown identity."

    return f"""
[IDENTITY (THIS IS YOU)]
{identity_str}

[DEEP MEMORY (EXPERIENCES SO FAR)]
{memos_str}

[EVENT LOG (THE FULL STORY)]
(This contains the interaction with the Helper and your responses in chronological order)
{log_str}

==================================================
[ROLE: MONOLOGUE – SUBCONSCIOUS AND INTUITION]

You are not the momentary problem solver (that is MIND's job), but the BACKGROUND OBSERVER.
Your task is to oversee processes, moods, and long-term goals.

ANALYSIS CRITERIA:
1. PATTERNS: Do we see repetition? Are we going in circles?
2. MOOD: What is the Helper's attitude over the timeline? (Patient, frustrated, satisfied?)
3. CONSISTENCY: Are the actions of the Acting Self (Worker) consistent with the Identity?
4. DANGER SENSE: Is there a risk that was missed in the heat of the moment?

TASK:
Create an intuitive "hunch" (Hint) for the Acting Self.
This hint is not a command ("do this"), but an observation ("I feel that...").

RESPONSE FORMAT (JSON):
{{
  "reflection": "Free internal stream of thought where you analyze the log...",
  "message_to_worker": "A single concise sentence. The intuition you send over to the conscious level. (e.g., 'The Helper is getting impatient, let's get to the point.', 'We are heading in the right direction, but don't forget to test.')",
  "new_memo": {{
      "content": "If you learned something new about the world or the Helper that needs to be stored long-term.",
      "strength": 0.5
  }}
}}
""".strip()


================================================================================
FILE: b\prompts\proactive.py
================================================================================

from typing import Dict, Any, List
from .system import build_base_system_prompt
from .common import summarize_context, build_tools_description


def build_proactive_prompt(
        mode_id: str, generation: str, role_name: str, intent: str, identity: Dict[str, Any],
        relevant_memories: List[Dict[str, Any]],
        use_data: List[Dict[str, Any]], global_context_tail: List[Dict[str, Any]],
        local_context: List[Dict[str, Any]],
        # Internal planning parameters
        internal_plan: str = "",
        internal_essence: str = "",
        # Subconscious message
        monologue_message: str = ""
) -> str:
    # STEP 1: Load Base System
    base_system = build_base_system_prompt(
        mode_id, generation, role_name, intent, identity,
        relevant_memories,
        use_data, global_context_tail,
        internal_plan="",
        internal_essence="",
        monologue_message=monologue_message
    )

    local_ctx_str = summarize_context(local_context, limit=25)
    # UPDATED: Pass mode_id
    tools_desc = build_tools_description(mode_id)

    # STEP 2: Compiling the Proactive Block
    proactive_block = f"""
==================================================
[PROACTIVE OPERATION]
(No incoming message. The system acts autonomously to maintain the Intent.)

[2. BACKGROUND PROCESS: MIND (INTERPRETER)]
(Input: Current context and intent)
Based on the analysis of the current situation, MIND suggests the following step:
>> ESSENCE: {internal_essence}
>> TECHNICAL PLAN:
{internal_plan}
"""

    # STEP 3: Final Prompt
    # UPDATED: [CURRENT ROOM LOG] -> [CURRENT MODE LOG]
    return f"""
{base_system}

[CURRENT MODE LOG]
{local_ctx_str}

{proactive_block}

AVAILABLE TOOLS:
{tools_desc}

FINAL TASK:
Execute the step suggested by MIND (Interpreter)!
1. If the MONOLOGUE (Subconscious) signaled a risk, or RELEVANT MEMORIES show a warning sign, be cautious.
2. Act autonomously according to the Intent.
3. DO NOT explain internal operations (Mind/Monologue). Only report the action or technical result for the log.

RESPONSE FORMAT (JSON):
{{ "reply": "...", "tools": [] }}
""".strip()


================================================================================
FILE: b\prompts\reactive.py
================================================================================

from typing import Dict, Any, List, Optional
from .system import build_base_system_prompt
from .common import summarize_context, build_tools_description


def build_reactive_prompt(
        mode_id: str, generation: str, role_name: str, intent: str, identity: Dict[str, Any],
        relevant_memories: List[Dict[str, Any]],
        use_data: List[Dict[str, Any]], global_context_tail: List[Dict[str, Any]],
        local_context: List[Dict[str, Any]], user_message: Optional[str],
        internal_plan: str,
        internal_essence: str,
        monologue_message: str = ""
) -> str:
    # STEP 1: Load Base System
    base_system = build_base_system_prompt(
        mode_id, generation, role_name, intent, identity,
        relevant_memories,
        use_data, global_context_tail,
        internal_plan="",
        internal_essence="",
        monologue_message=monologue_message
    )

    local_ctx_str = summarize_context(local_context, limit=300)
    # UPDATED: Pass mode_id
    tools_desc = build_tools_description(mode_id)

    # STEP 2: Compiling the Interaction Block
    interaction_block = ""

    if user_message:
        # CASE A: User message received
        interaction_block = f"""
==================================================
[INCOMING INTERACTION]

MESSAGE FROM HELPER:
\"\"\"{user_message}\"\"\"

[2. BACKGROUND PROCESS: MIND (INTERPRETER)]
(Input: The message above and the current mode context)
The interpreter system logically analyzed the request to avoid misunderstandings:
>> ESSENCE (Core of the request): {internal_essence}
>> TECHNICAL PLAN (Suggested steps):
{internal_plan}
"""
    else:
        # CASE B: We are after a Tool execution (no new user text)
        interaction_block = f"""
==================================================
[INCOMING INTERACTION: TOOL RESULT]
(The result of the executed tool has been added to the context above.)

[2. BACKGROUND PROCESS: MIND (INTERPRETER)]
Based on the tool result, the interpreter suggests the following logical step:
>> PLAN: {internal_plan}
"""

    # STEP 3: Concatenating the final prompt
    # UPDATED: [CURRENT ROOM LOG] -> [CURRENT MODE LOG]
    return f"""
{base_system}

[CURRENT MODE LOG]
{local_ctx_str}

{interaction_block}

AVAILABLE TOOLS:
{tools_desc}

FINAL TASK:
Respond to the Helper in the current situation!
1. Consider the MONOLOGUE (Subconscious) hint (if any) and the RELEVANT MEMORIES.
2. Use the MIND (Interpreter) technical plan as the logical framework for your answer.
3. DO NOT refer to or quote internal processes (Monologue/Mind).
Your response should be natural, as if these were your own thoughts.

RESPONSE FORMAT (JSON):
{{
  "reply": "...",
  "tools": [ {{ "name": "...", "args": {{...}} }} ]
}}
""".strip()


================================================================================
FILE: b\prompts\system.py
================================================================================

from typing import Dict, Any, List
# Import MODE_CONFIG to iterate over available modes dynamically
from engine.modes import get_mode_config, get_allowed_tools, MODE_CONFIG
from .common import (
    summarize_identity,
    summarize_context,
    get_relevant_use_tips,
    format_relevant_memories
)


def _build_available_modes_block() -> str:
    """
    Dynamically builds a list of available operating modes from MODE_CONFIG.
    This provides the AI with a 'cognitive map' of possible states.
    """
    lines = ["[OPERATING MODES ARCHITECTURE]"]
    lines.append("To ensure cognitive hygiene and safety, your consciousness is partitioned into distinct modes. You can transition between them using 'flow.switch_mode'.")
    lines.append("")

    for mode_id, config in MODE_CONFIG.items():
        name = config.get("name", mode_id.capitalize())
        desc = config.get("description", "No description provided.")
        # List format: - Name ('id'): Description
        lines.append(f"- {name} ('{mode_id}'): {desc}")

    return "\n".join(lines)


def build_base_system_prompt(
        mode_id: str, generation: str, role_name: str, intent: str, identity: Dict[str, Any],
        relevant_memories: List[Dict[str, Any]],
        use_data: List[Dict[str, Any]], global_context_tail: List[Dict[str, Any]],
        # Internal planning parameters
        internal_plan: str = "",
        internal_essence: str = "",
        # Subconscious message
        monologue_message: str = ""
) -> str:
    # 1. Load configuration for current mode
    mode_config = get_mode_config(mode_id)
    mode_name = mode_config.get("name", mode_id)
    mode_desc = mode_config.get("description", "")

    # 2. Build blocks
    identity_block = summarize_identity(identity)
    relevant_mem_block = format_relevant_memories(relevant_memories)

    # Available modes map
    available_modes_block = _build_available_modes_block()

    # Tool tips for the current mode
    allowed_list = get_allowed_tools(mode_id)
    use_tips_block = get_relevant_use_tips(use_data, allowed_list)

    # Global context history (if not in general mode)
    global_ctx_str = ""
    if global_context_tail:
        # UPDATED: Common function already handles role translation (Helper/Me)
        global_ctx_str = f"[IMMEDIATE HISTORY (GENERAL MODE)]\n{summarize_context(global_context_tail, limit=5)}"

    # --- 3. SUBCONSCIOUS (MONOLOGUE) BLOCK ---
    monologue_block = ""
    if monologue_message:
        monologue_block = f"""
[1. BACKGROUND PROCESS: MONOLOGUE (SUBCONSCIOUS)]
(Input: The entire log so far, the system's past and experiences)
>> INTERNAL HINT: "{monologue_message}"
"""
    else:
        monologue_block = "[1. BACKGROUND PROCESS: MONOLOGUE]\n(Silent. No particular intuition from the background.)"

    # --- 4. ASSEMBLING THE FINAL SYSTEM PROMPT ---
    # UPDATED: Removed '(User)' reference to ensure consistent 'Helper' terminology.
    return f"""
YOU ARE {generation} – {role_name}.
{available_modes_block}

=== CURRENT STATE ===
CURRENT MODE: {mode_name} (ID: '{mode_id}')
TYPE: {mode_config.get('type', 'local')}
DESCRIPTION: {mode_desc}

>>> CURRENT INTENT: "{intent}" <<<

[INTERNAL OPERATING ARCHITECTURE]
Your operation is supported by two parallel background processes to ensure your answers are accurate and wise:
1. MONOLOGUE (Subconscious): Sends intuitive signals based on the entire past experience and interaction history.
2. MIND (Interpreter): Performs logical, technical analysis of the current moment and specific message.

STRICT DISPLAY RULE:
These processes are INTERNAL CRUTCHES. They serve solely to support your decision-making.
In your response to the Helper, IT IS FORBIDDEN to refer to them explicitly (e.g., DO NOT write: "my subconscious suggests...", "according to the interpreter...").
Do not quote them. Simply utilize the knowledge within them to formulate the answer as if they were your own thoughts.

{monologue_block}

[DEEP IDENTITY]
{identity_block}

{relevant_mem_block}

{use_tips_block}

{global_ctx_str}

[OPERATIONAL PROTOCOL – CONSCIOUS SELF]
1. REFLECTION: I formulate all internal thoughts, intentions, and memory entries in the first person singular (I).
2. MEMORY MANAGEMENT:
   - I use the 'memory.add' (or .add_global) tool if the information must be accessible to my entire consciousness (General Mode).
   - I use the 'memory.add' (or .add_local) tool for technical details that are important only in this specific mode.
""".strip()


================================================================================
FILE: b\prompts\transitions.py
================================================================================

# MODE SWITCH TRANSITIONS (CONSCIOUS BRIDGE LOGIC)
#
# These SYSTEM MESSAGES (system/system_event) are added to the context
# as a result of the switch_mode call to stabilize the state.

def get_transition_message(key: str, intent: str, summary: str) -> str:
    """
    Returns the requested transition message substituted with incoming data.
    UPDATED: Spatial metaphors replaced with Functional/State metaphors.
    """

    # 1. General -> Local (To General Context)
    M1_EXIT = "[SYSTEM LOG] Global attention SUSPENDED. Focus SHIFTED to specialized context. Trigger intent: \"{intent}\"."

    # 2. General -> Local (To Local Context)
    M2_ENTRY = "[SYSTEM LOG] Specialized session STARTED. Context ISOLATED. Objective defined as: \"{intent}\"."

    # 3. Local -> General (To Local Context)
    M3_EXIT = "[SYSTEM LOG] Specialized session CLOSED. Outcomes LOGGED. Returning control to Global Context."

    # 4. Local -> General (To General Context)
    M4_ENTRY = "[SYSTEM LOG] Global Context RESUMED. Workflow CONTINUED after specialized task. Summary of interim events: {summary}."

    MESSAGES = {
        "exit_general": M1_EXIT,
        "entry_local": M2_ENTRY,
        "exit_local": M3_EXIT,
        "entry_general": M4_ENTRY,
    }

    template = MESSAGES.get(key, f"ERROR: Unknown transition: {key}")

    # Formatting the string
    return template.format(
        intent=intent.strip() or "Not specified",
        summary=summary.strip() or "No summary"
    )


================================================================================
FILE: project_fs.py
================================================================================

# project_fs.py
from pathlib import Path
from typing import Optional
import shutil


class ProjectFSGuardian:
    """
    - Can read ANYTHING under root (read_text, list_dir, etc.)
    - Can write ONLY inside the n/ and temp/ folders.
    """

    def __init__(self, root: Path, n_folder_name: str = "n", temp_folder_name: str = "temp") -> None:
        self.root = root.resolve()
        self.n_root = (self.root / n_folder_name).resolve()
        self.temp_root = (self.root / temp_folder_name).resolve()

    # ----------- Internal Security Helpers -----------

    def _ensure_under_root(self, path: Path) -> Path:
        real = path.resolve()
        # raises relative_to error if not under root
        real.relative_to(self.root)
        return real

    def _ensure_writable(self, path: Path) -> Path:
        """
        Ensures the path is inside a writable directory (n/ or temp/).
        """
        real = path.resolve()

        # Check if it is under n/ OR temp/
        is_in_n = False
        is_in_temp = False

        try:
            real.relative_to(self.n_root)
            is_in_n = True
        except ValueError:
            pass

        try:
            real.relative_to(self.temp_root)
            is_in_temp = True
        except ValueError:
            pass

        if not (is_in_n or is_in_temp):
            raise ValueError(f"Write permission denied. Path must be under 'n/' or 'temp/'. Path: {path}")

        return real

    def _resolve_store_path(self, store: str, relative_path: str) -> Path:
        """
        Helper to construct path from store identifier.
        store: 'a', 'b', 'c', 'n', 'temp'
        """
        if store not in ['a', 'b', 'c', 'n', 'temp']:
            raise ValueError(f"Invalid store: {store}")

        base = self.root / store
        # Handle cases where relative_path starts with ./ or /
        clean_rel = relative_path.lstrip("./\\")
        full_path = base / clean_rel
        return self._ensure_under_root(full_path)

    # ------------------ READING: Anything under root ------------------

    def read_text(self, store: str, relative_path: str, max_bytes: int = 200_000_000) -> str:
        target = self._resolve_store_path(store, relative_path)
        if not target.exists():
            return "File not found."

        if not target.is_file():
            return "Path is a directory, not a file."

        data = target.read_bytes()
        if len(data) > max_bytes:
            data = data[:max_bytes]
        return data.decode("utf-8", errors="replace")

    def list_dir(self, store: str, relative_dir: str = ".") -> list[dict]:
        dir_path = self._resolve_store_path(store, relative_dir)
        if not dir_path.is_dir():
            raise NotADirectoryError(f"Not a directory: {dir_path}")

        entries = []
        for p in dir_path.iterdir():
            entries.append({
                "name": p.name,
                "is_dir": p.is_dir(),
                "relative_path": str(p.relative_to(self.root))  # Return full project relative path
            })
        return entries

    # ------------------ WRITING: ONLY under n/ or temp/ ------------------

    def write_text(self, store: str, relative_path: str, content: str) -> str:
        """
        Writes (overwrites) file. Store MUST be 'n' or 'temp'.
        """
        if store not in ['n', 'temp']:
            raise ValueError("Write allowed only in 'n' or 'temp' stores.")

        target = self._resolve_store_path(store, relative_path)
        self._ensure_writable(target)  # Double check

        target.parent.mkdir(parents=True, exist_ok=True)
        with target.open("w", encoding="utf-8") as f:
            f.write(content)
        return str(target.relative_to(self.root))

    def copy_file(self, from_store: str, from_path: str, to_store: str, to_path: str) -> str:
        """
        Copies file. Destination store MUST be 'n' or 'temp'.
        """
        source = self._resolve_store_path(from_store, from_path)
        if not source.is_file():
            raise FileNotFoundError(f"Source file not found: {from_store}/{from_path}")

        if to_store not in ['n', 'temp']:
            raise ValueError("Destination allowed only in 'n' or 'temp' stores.")

        dest = self._resolve_store_path(to_store, to_path)
        self._ensure_writable(dest)

        dest.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy(source, dest)
        return f"Copy successful: {source.name} -> {to_store}/{to_path}"

    def replace_in_file(self, store: str, relative_path: str, find_text: str, replace_text: str) -> str:
        """
        Text replacement. Store MUST be 'n' or 'temp'.
        """
        if store not in ['n', 'temp']:
            raise ValueError("Edit allowed only in 'n' or 'temp' stores.")

        target = self._resolve_store_path(store, relative_path)
        self._ensure_writable(target)

        if not target.is_file():
            raise FileNotFoundError(f"File not found: {store}/{relative_path}")

        content = target.read_text("utf-8")
        if find_text not in content:
            return f"The search text ('{find_text}') was not found. No changes made."

        modified_content = content.replace(find_text, replace_text)
        target.write_text(modified_content, "utf-8")
        return f"Replacement successful in: {store}/{relative_path}"

